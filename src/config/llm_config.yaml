# LLM Provider Configuration
# Switch providers by changing active_provider

# Active provider: "openai" or "ollama"
active_provider: "openai"

# ==================== OpenAI / ChatGPT ====================
openai:
  api_key_env: "OPENAI_API_KEY"
  base_url: "https://api.openai.com/v1"

  # Models
  models:
    chat: "gpt-4o-mini"
    embeddings: "text-embedding-3-small"

  # Parameters
  temperature: 0.1
  max_tokens: 2000
  timeout: 30

  # Costs ($/1M tokens)
  costs:
    chat_input: 0.15
    chat_output: 0.60
    embeddings: 0.02

# ==================== Ollama (Local) ====================
ollama:
  base_url: "http://localhost:11434"

  # Models (must be installed via: ollama pull <model>)
  models:
    chat: "llama3.1:8b"
    embeddings: "nomic-embed-text"

  # Parameters
  temperature: 0.1
  timeout: 60
  stream: false

  # Costs (local = free!)
  costs:
    chat_input: 0.0
    chat_output: 0.0
    embeddings: 0.0

# ==================== General Settings ====================
general:
  retry_attempts: 3
  retry_delay: 1.0
  log_requests: true
