{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Извлечение фактов из научной статьи с помощью spaCy\n",
    "\n",
    "Этот notebook демонстрирует извлечение фактов (established knowledge) из научной статьи `2510.04749v1.pdf` используя библиотеку spaCy.\n",
    "\n",
    "**Цель:** Получить список фактов из секций Introduction и Background статьи."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "## 1. Установка и импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка зависимостей (запустить один раз)\n",
    "# !pip install spacy PyMuPDF\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys\n",
    "from itertools import count\n",
    "\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "# Добавляем корень проекта в путь для импорта модулей\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.parsers import PDFParser\n",
    "from src.models import Entity, EntityType"
   ],
   "id": "eac71c34c5b0610"
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8",
   "metadata": {},
   "source": [
    "## 2. Загрузка spaCy модели и парсинг PDF"
   ]
  },
  {
   "cell_type": "code",
   "id": "f6a7b8c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T06:44:14.660348Z",
     "start_time": "2025-10-12T06:44:14.476206Z"
    }
   },
   "source": [
    "# Загрузка spaCy модели\n",
    "print(\"Loading spaCy model...\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"✓ Model loaded\")\n",
    "\n",
    "# Путь к PDF файлу\n",
    "pdf_path = project_root / \"docs\" / \"articles\" / \"2510_04749v1\" / \"2510.04749v1.pdf\"\n",
    "print(f\"\\nPDF path: {pdf_path}\")\n",
    "print(f\"File exists: {pdf_path.exists()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spaCy model...\n",
      "✓ Model loaded\n",
      "\n",
      "PDF path: /Users/ivanartemov/PycharmProjects/AAIAA/docs/articles/2510_04749v1/2510.04749v1.pdf\n",
      "File exists: True\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T06:48:05.322226Z",
     "start_time": "2025-10-12T06:48:05.284969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Парсинг PDF\n",
    "print(\"Parsing PDF...\")\n",
    "parser = PDFParser()\n",
    "parsed_doc = parser.parse(str(pdf_path))\n",
    "\n",
    "print(f\"\\n✓ Parsing completed in {parsed_doc.parse_time:.2f}s\")\n",
    "print(f\"  Pages: {parsed_doc.page_count}\")\n",
    "print(f\"  Words: {parsed_doc.word_count}\")\n",
    "print(f\"  Sections detected: {list(parsed_doc.sections.keys())}\")"
   ],
   "id": "d5b521dcbb51d80c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing PDF...\n",
      "\n",
      "✓ Parsing completed in 0.03s\n",
      "  Pages: 10\n",
      "  Words: 3673\n",
      "  Sections detected: []\n",
      "ParsedDocument(text='LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows ⋆ Samy Ateia1 , Udo Kruschwitz1 , Melanie Scholz2, Agnes Koschmider2 , and Moayad Almohaishi2 1 University of Regensburg, Universitätsstraße 31, 93053 Regensburg, Germany {udo.kruschwitz,samy.ateia}@ur.de 2 University of Bayreuth, Universitätsstraße 30, 95447 Bayreuth, Germany {melanie.scholz,agnes.koschmider,moayad.almohaishi}@uni-bayreuth.de Abstract. The increasing volume of scholarly publications requires ad- vanced tools for efficient knowledge discovery and management. This paper introduces ongoing work on a system using Large Language Mod- els (LLMs) for the semantic extraction of key concepts from scientific documents. Our research, conducted within the German National Re- search Data Infrastructure for and with Computer Science (NFDIx CS) project, seeks to support FAIR (Findable, Accessible, Interoperable, and Reusable) principles in scientific publishing. We outline our explorative work, which uses in-context learning with various LLMs to extract con- cepts from papers, initially focusing on the Business Process Manage- ment (BPM) domain. A key advantage of this approach is its potential for rapid domain adaptation, often requiring few or even zero examples to define extraction targets for new scientific fields. We conducted technical evaluations to compare the performance of commercial and open-source LLMs and created an online demo application to collect feedback from an initial user-study. Additionally, we gathered insights from the com- puter science research community through user stories collected during a dedicated workshop, actively guiding the ongoing development of our future services. These services aim to support structured literature re- views, concept-based information retrieval, and integration of extracted knowledge into existing knowledge graphs. Keywords: Large Language Models · Information Extraction · Scien- tific Publishing · Digital Libraries · FAIR Principles · Knowledge Graphs. ⋆Published in TPDL 2025, New Trends in Theory and Practice of Digital Libraries, Communications in Computer and Information Science, vol 2694. DOI 10.1007/978- 3-032-06136-2_9. This PDF is the author-prepared camera-ready version corre- sponding to the accepted manuscript and supersedes the submitted version that was inadvertently published as the version of record. ar Xiv:2510.04749v1 [cs.DL] 6 Oct 2025 2 S. Ateia et al. 1 Introduction The scientific publication landscape is booming with global annual publication growing by 59% according to the NSF3 and more than one million articles be- ing published per year in biomedicine and life sciences alone [6]. This rise in publications makes it harder for scientists to stay on top of their field, while also limiting the discoverability of their publications as they have to compete with others for visibility. Digital transformation and new tools such as LLMs can accelerate that problem, but also have the potential to assist scientists and publishing platforms in managing these challenges. In computer science, publications are often accompanied by software arti- facts and datasets for reproducibility, but their management frequently lacks standardization and fails to meet FAIR principles. The National Research Data Infrastructure for and with Computer Science (NFDIx CS) project4 addresses this by creating an infrastructure to implement FAIR principles [21] for CS research outputs in Germany [5]. But to make these artifacts findable it is necessary to link them to relevant semantic information from the publication text itself, for example, research questions or methods so that they are discoverably as related work by other scientists. Our project, situated within the NFDIx CS initiative, aims to develop tools to exactly address this problem. We leverage Large Language Models (LLMs) for the semantic analysis of scientific text, with the goal of enhancing the FAIR principles for scholarly literature. Specifically, we aim to: – Develop robust methods for automatically extracting key semantic concepts (e.g., research questions, methodologies, findings) from scientific papers. – Explore mechanisms for structuring these extracted concepts to improve the organization and distribution of digital content, potentially linking them to knowledge graphs. – Design and prototype services, informed by community needs, that use this structured information to support researchers in their workflows. This short paper presents our preliminary findings and outlines how user-driven requirements are shaping the trajectory of our research towards practical appli- cations. We publish a demo system alongside its source code under a permissive license5 alongside the results of our user workshops, and plan to maintain this practice for future services. 2 Related Work In this work, we explore the use of LLMs for knowledge extraction to improve scientific workflows within digital libraries and beyond. We review related efforts 3 https://web.archive.org/web/20250507134337/https://www.ncses.nsf.gov/ pubs/nsb202333/executive-summary 4 https://nfdixcs.org/ 5 CC BY 4.0 LLM Info Extraction for Research Workflows 3 in three key areas: platforms for scientific literature analysis, knowledge graphs for structuring scientific information, and the use of LLMs for information ex- traction. 2.1 Platforms for Scientific Literature Analysis Several platforms exist that use natural language processing (NLP) based on language models to highlight relevant information from scientific text, there- fore assisting in navigating the considerable volume of publications. Semantic Scholar [2], for example, employs AI to provide summaries (TLDRs) and iden- tify influential citations, while Elicit uses a systematic review inspired work- flow, leveraging LLMs to synthesize findings from multiple papers in response to a user’s query [20]. Scite.ai focuses specifically on citation context, classifying whether a citation supports, disputes, or merely mentions a claim [9]. While these platforms offer similar flexible LLM-based question answering tools, they do not currently offer the use of predefined domain-specific questions and mostly require a paid subscription to be fully utilized. With our approach, we want to potentially offer higher accuracy and user guidance through curated extraction targets and examples serving specialized communities. 2.2 Knowledge Graphs for Structuring Scientific Knowledge Structuring scientific knowledge in a machine-readable format has long been a goal of the scientific community. The Open Research Knowledge Graph (ORKG) [8] is a prominent initiative aiming to represent the content of research papers as structured data. By describing papers through their contributions, methods, and findings, the ORKG facilitates systematic comparisons and reviews. Other notable examples are the discontinued Microsoft Academic Graph (MAG) [18] that was succeeded by Open Alex [13] or Sci KGraph [17]. However, curating such knowledge graphs often requires significant manual effort from researchers. Most recently, ORKG Ask was introduced, which offers the possibility to create ad-hoc comparison tables using information extraction with LLMs [11]. In our work, we want to build on that approach and take it a step further. Instead of just having users query questions on a set of retrieved papers, we explore how curated questions from domain-experts can be leveraged to prefill knowledge graph input templates for users. This complements the KG vision by lowering the barrier to entry and scaling up content acquisition. Embedding and indexing the extracted information in separate fields could lead to improved semantic search, by enabling users to search for papers with similar research questions or algorithms. 2.3 LLMs for Information Extraction in Science In-context learning with LLMs describes the ability of these models to solve problems that they have not explicitly been trained on, by just giving the model 4 S. Ateia et al. an abstract description of the problem (Zero-Shot) or several examples (Few- Shot) in their input context. These approaches were first popularized with LLMs like GPT-3 [3] and enable their use in domains where limited, or no training data is available. Recent LLMs such as the Google Gemini series6 or Open AIs GPT- 4.17 have pushed the size of the available context up to 1 million input tokens. Which makes it feasible to extract information from large text sources in a single step. These properties can be used in retrieval augmented generation (RAG) [16] systems that ground the knowledge of these models in relevant texts. Systems such as CORE-GPT have shown the usefulness of such approaches in questions answering across multiple scientific domains [12]. In our work, we explore both zero- and few-shot learning for extracting pre- defined semantic information from scientific texts, that can then be used to facilitate scientific knowledge discovery and publication workflows. 3 Methodology: LLM-Based Concept Extraction Our system uses an LLM to extract semantic information from scientific doc- uments. The demo UI allows a user to upload a paper and pose predefined or custom questions. The LLM then processes the document and a prompt to iden- tify and return relevant information or synthesized answers (Figure 1). Fig. 1. LLM-based demo extraction pipeline. 3.1 In-context Learning for Rapid Domain Adaptation We leverage the in-context learning of modern LLMs for rapid domain adap- tation. By providing instructions and a few examples to guide extraction, our 6 https://web.archive.org/web/20250607225206/https://blog.google/ technology/ai/google-gemini-next-generation-model-february-2024/ 7 https://web.archive.org/web/20250612080402/https://openai.com/index/ gpt-4-1/ LLM Info Extraction for Research Workflows 5 method avoids the need for the extensive, domain-specific datasets required by traditional supervised techniques. In our evaluations we tested two modes: A few-shot mode where we supplied three in-domain examples each consisting of 1. the full text of the document; 2. the domain-specific information extraction questions, 3. the instructions, 4. the manually crafted ideal answer from the document. In addition, we tested a zero- shot mode where we only supplied the instruction and the full text of the PDF. The zero-shot mode formed our baseline for evaluation, but could also be used to offer the flexibility to the user to pose their own extraction questions against a document or a set of retrieved documents. The few-shot mode aligns the model with the style of the manual annotators, overcoming limited or missing instructions in the way the question was posed. This mode could be used in set- tings where predefined questions and predictable answer formats are important, for example when prefilling forms for later knowledge graph mapping. We tested four different models: Qwen 2.5 72B instruct [14], Llama 3.3 70B instruct8 [7], Gemini 1.5 Flash 002, Gemini 1.5 Flash 8B 001 [4]. Llama and Qwen were accessed via openrouter.ai while the Gemini models were accessed via the official Google API. The instructions used chain of thought prompting [19] to generate a reason- ing beside relevant context and the final answer. The exact zero-shot prompt can be seen in Listing 1.1. Listing 1.1. Zero-shot prompt example in Python Extract the information answering the following question from the text: Question: ‘‘‘{question}‘‘‘ Text: ‘‘‘{text}‘‘‘ Return a JSON object in the following format: {{ \"reasoning \": \"<think step by step and write down your reasoning >\", \"context \": \"<contains all relevant context from the text >\", \"answer \": \"<one concise answer to the question for example: yes/no/none , or a word or multiple words >\" }} Try to be concise and limit your reasoning , answer , and the extracted context to max 500 words. 3.2 Dataset and Domain The initial development and a preliminary evaluation were carried out on a corpus of 122 scientific papers from the Business Process Management (BPM) conferences (2019–2023)9. This domain was selected due to the availability of domain experts who are actively constructing a knowledge graph in this area. 8 https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/ 9 https://bpm-conference.org/conferences/ 6 S. Ateia et al. Key concepts were manually annotated in the papers to establish a gold standard for evaluating extraction performance. 3.3 Extraction Example An example of the extraction process for the Target Concept of a \"Research Question\" would involve posing the Query: \"What is the explicitly stated re- search question for the paper?\". The system might then return an Example Extracted Answer like: \"How to decide which processes need to be analyzed in detail to determine if changes are necessary.\" This methodology is highly flexible, allowing us to target a wide array of semantic information within scientific texts with high adaptability, as only lim- ited domain expert involvement is needed to create a few examples for each information item. 3.4 Demo System To showcase the ability of the tool and collect initial user feedback, we built a demo UI using the Gradio framework[1] around our approach. The demo system is available online10(user:demo, pw:demo) and the source code for this system is available on Git Hub11. 3.5 User Feedback In a pilot user study, we collected initial user-feedback with the demo system through a questionnaire after instructing a panel of users to choose one paper from a selection of business processing domain papers, upload it to the tool and select any questions that they were interested in. At a separate workshop with around 30 participants from different computer- science fields, we collected user-stories that they would like to be solved by the offered and demonstrated technology. 4 Results We evaluated model performance against a gold-standard dataset of 122 manu- ally annotated papers. We used three metrics based on the target type: 1. For categorical targets (15 targets, 1121 annotations), we used Exact Acc, a token-set accuracy based on a Jaccard similarity with a threshold of 0.8 [10,15]. 2. For binary targets (13 targets, 984 annotations), we used the macro-averaged F1-score (Bin F1). 10 https://demo-d3.nfdixcs.org/ 11 https://github.com/Samy Ateia/nfdixcs-d3-knowledge-extraction-demo LLM Info Extraction for Research Workflows 7 3. For free-text targets (4 targets, 488 annotations), we assessed semantic equiv- alence with the F1-score from BERTScore [22]. The Overall score in Table 1 is the unweighted arithmetic mean of these three metrics. Table 1. Model comparison on the paper-coding benchmark (higher = better) Model Exact Acc Bin F1 BERT_F1 Overall qwen-2.5-72b (3-shot) 0.249 0.594 0.863 0.569 qwen-2.5-72b (0-shot) 0.219 0.514 0.887 0.540 llama-3.3-70b (0-shot) 0.212 0.556 0.877 0.548 gemini-1.5-flash-002 (3-shot) 0.246 0.330 0.893 0.490 gemini-1.5-flash-002 (0-shot) 0.183 0.390 0.883 0.486 gemini-1.5-flash-8b-001 (0-shot) 0.171 0.345 0.881 0.466 gemini-1.5-flash-8b-001 (3-shot) 0.180 0.148 0.897 0.408 BERT_F1 scores near 0.90 indicate strong semantic alignment on free-text fields, whereas binary indicators show moderate performance (best Bin F1 = 0.59) and exact categorical extraction remains limited (Exact Acc < 0.25). 4.1 User Study & Workshop Feedback from our pilot user study on the prototype demo UI was positive (88% satisfaction with extracted concepts), indicating the potential utility of the approach. While some feedback was UI related (hiding advanced configuration like few-shot examples, wanting more expert configuration), a main point was that the traceability of the extracted information should be improved. In a separate workshop with around 30 computer-science researchers, we col- lected 56 user-stories. 38 of these focused on the task of literature research and comparison, 8 on assistance while writing papers, 3 on support in the review process, 3 were directed towards software development and 4 were unique. Over- all, it became clear that the users want to go beyond just extracting concepts from one specific paper and compare the extracted information from multiple papers instead. The full categorized list is available in our repository12. 5 Discussion The results of our technical evaluation and user-studies, while preliminary, pro- vide valuable direction for the development of our future services. Few-shot examples seemed to improve the performance of the models in tasks where specific categorial answers were needed and on the free-text extractions 12 https://github.com/Samy Ateia/nfdixcs-d3-knowledge-extraction-demo 8 S. Ateia et al. measured by BERTScore. But on the binary classification task, the performance decreased. This could be explained by a class bias introduced via the few-shot examples, while on the textual extractions the examples might have informed the model better about the expected format of the answers. Using the full-text of documents in few-shot examples is costly and poten- tially increases noise. We are working on exploring the impact of more and shorter examples and selecting ideal examples for specific extraction target types. The open-weight models Qwen 2.5-72B-Instruct and Llama 3.3-70B-Instruct seemed to perform better than the commercial models that we tested. For the Gemini 1.5-flash-8b model, this is most likely explained by the difference in size. The size of the normal Gemini 1.5-flash model is unknown but given our results and the cost and speed we suspect that it is also smaller than the 70 billion parameter models that we compared them to. From the feedback that we collected through our pilot user study and the discussion in a later workshop, it became clear that there is a need for better transparency and traceability. Ideally, highlighting the text passages that inform an extracted information items in the source document. Even though there are commercial services available that are similar to our tool, our contribution can inform researchers and professionals that want to offer customizable domain-specific services to their users. We demonstrate the feasibility of in-context learning and open-weights models for these use-cases and publish our code to boost independent development of transparent services. 6 Conclusion and Ongoing Work We confirmed the potential of current LLMs to summarize and extract domain- specific information from scientific text. Through in-context learning, these mod- els can be quickly adapted to specific scientific domains and facilitate the transfer of expert knowledge between researchers by highlighting and comparing key as- pects of their work. Through our user-centric approach, we collected valuable feedback and user- stories that can guide the development of current and future services. Notable transparency and the need that services enable the user to verify LLM generated output by tracing summarized information back to the source text. Our ongoing work will focus on exploring embedding-based retrieval on the extracted structured information, therefore overcoming the arbitrary chunking issue that limits semantic relevance in vector search. We’re also exploring how our approach can be integrated in the publishing process, prefilling templates for knowledge graph mapping e.g., for ORKG. Making it easier for authors to fill out forms that facilitate the discoverability of their work. Overall, our work highlights the potential of LLMs to improve the publishing process and discoverability of scientific information in digital libraries and be- yond. Its main contribution is demonstrating the practical integration of these models into a user-focused, open-source system designed to tackle real-world challenges, rather than proposing a novel extraction algorithm itself. LLM Info Extraction for Research Workflows 9 Acknowledgments. We thank the anonymous reviewers for their valuable feed- back. This work is funded by the German Research Foundation (DFG) as part of the NFDIx CS consortium (Grant number: 501930651). Disclosure of Interests. The authors have no competing interests to declare that are relevant to the content of this article. References 1. Abid, A., Abdalla, A., Abid, A., Khan, D., Alfozan, A., Zou, J.: Gradio: Hassle- free sharing and testing of ml models in the wild (2019), https://arxiv.org/abs/ 1906.02569 2. Ammar, W., Groeneveld, D., Bhagavatula, C., Beltagy, I., Crawford, M., Downey, D., Dunkelberger, J., Elgohary, A., Feldman, S., Ha, V., et al.: Construction of the literature graph in semantic scholar. ar Xiv preprint ar Xiv:1805.02262 (2018) 3. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee- lakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. Advances in neural information processing systems 33, 1877–1901 (2020) 4. Georgiev, P., et al.: Gemini 1.5: Unlocking multimodal understanding across mil- lions of tokens of context (2024), https://arxiv.org/abs/2403.05530 5. Goedicke, M., Lucke, U.: Research Data Management in Computer Science- NFDIx CS Approach. In: INFORMATIK 2022. pp. 1317–1328. Gesellschaft für In- formatik, Bonn (2022) 6. González-Márquez, R., Schmidt, L., Schmidt, B.M., Berens, P., Kobak, D.: The landscape of biomedical research. Patterns 5(6), 100968 (2024). https://doi.org/ https://doi.org/10.1016/j.patter.2024.100968 7. Grattafiori, A., et al.: The Llama 3 Herd of Models (2024), https://arxiv.org/ abs/2407.21783 8. Jaradeh, M.Y., Oelen, A., Prinz, M., Stocker, M., Auer, S.: Open research knowl- edge graph: a system walkthrough. In: Digital Libraries for Open Knowledge: 23rd International Conference on Theory and Practice of Digital Libraries, TPDL 2019, Oslo, Norway, September 9-12, 2019, Proceedings 23. pp. 348–351. Springer (2019) 9. Lund, B., Shamsi, A.: Examining the use of supportive and contrasting citations in different disciplines: a brief study using Scite (scite. ai) data. Scientometrics 128(8), 4895–4900 (2023) 10. Mann, W., Augsten, N., Bouros, P.: An empirical evaluation of set similarity join techniques. Proceedings of the VLDB Endowment 9(9), 636–647 (2016) 11. Oelen, A., Jaradeh, M.Y., Auer, S.: ORKG ASK: A Neuro-symbolic Scholarly Search and Exploration System. ar Xiv preprint ar Xiv:2412.04977 (2024) 12. Pride, D., Cancellieri, M., Knoth, P.: CORE-GPT: Combining open access re- search and large language models for credible, trustworthy question answering. In: International Conference on Theory and Practice of Digital Libraries. pp. 146–159. Springer (2023) 13. Priem, J., Piwowar, H., Orr, R.: Open Alex: A fully-open index of scholarly works, authors, venues, institutions, and concepts. ar Xiv preprint ar Xiv:2205.01833 (2022) 14. Qwen, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Tang, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, 10 S. Ateia et al. Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., Qiu, Z.: Qwen2.5 Technical Report (2025), https://arxiv.org/abs/2412.15115 15. Schmidt, L., Mutlu, A.N.F., Elmore, R., Olorisade, B.K., Thomas, J., Higgins, J.P.: Data extraction methods for systematic review (semi) automation: Update of a living systematic review. F1000Research 10, 401 (2025) 16. Shuster, K., Poff, S., Chen, M., Kiela, D., Weston, J.: Retrieval Augmentation Reduces Hallucination in Conversation. In: Findings of the Association for Com- putational Linguistics: EMNLP 2021. pp. 3784–3803 (2021) 17. Tosi, M.D.L., dos Reis, J.C.: Sci KGraph: A knowledge graph approach to structure a scientific field. Journal of Informetrics 15(1), 101109 (2021). https://doi.org/https://doi.org/10.1016/j.joi.2020.101109, https://www.sciencedirect.com/science/article/pii/S175115772030626X 18. Wang, K., Shen, Z., Huang, C., Wu, C.H., Dong, Y., Kanakia, A.: Microsoft aca- demic graph: When experts are not enough. Quantitative Science Studies 1(1), 396–413 (2020) 19. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E.H., Le, Q.V., Zhou, D.: Chain-of-thought prompting elicits reasoning in large language models. In: Proceedings of the 36th International Conference on Neural Information Processing Systems. NIPS ’22, Curran Associates Inc., Red Hook, NY, USA (2022) 20. Whitfield, S., Hofmann, M.A.: Elicit: AI literature review research assistant. Public Services Quarterly 19(3), 201–207 (2023) 21. Wilkinson, M.D., Dumontier, M., Aalbersberg, I.J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.W., da Silva Santos, L.B., Bourne, P.E., et al.: The FAIR Guiding Principles for scientific data management and stewardship. Scientific data 3(1), 1–9 (2016) 22. Zhang, T., Kishore, V., Wu, F., Weinberger, K.Q., Artzi, Y.: Bertscore: Evaluating text generation with bert. ar Xiv preprint ar Xiv:1904.09675 (2019)', sections={}, metadata={'title': 'LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows', 'author': 'Samy Ateia; Udo Kruschwitz; Melanie Scholz; Agnes Koschmider; Moayad Almohaishi', 'subject': '', 'keywords': '', 'creator': 'arXiv GenPDF (tex2pdf:)', 'producer': 'pikepdf 8.15.1', 'creation_date': '', 'modification_date': ''}, word_count=3673, page_count=10, parse_time=0.03432273864746094)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Извлечение фактов с помощью spaCy\n",
    "\n",
    "Факты обычно находятся в секциях Introduction и Background. Используем spaCy для:\n",
    "- Определения границ предложений\n",
    "- Pattern matching для утверждений\n",
    "- NER для научных терминов"
   ],
   "id": "40ab8bc2f06e74a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "\n",
    "# Паттерны для идентификации фактов (утверждений из литературы)\n",
    "fact_patterns = [\n",
    "    # Паттерны с цитированием\n",
    "    r'\\[\\d+[,\\d\\s-]*\\]',  # [1], [1,2], [1-3]\n",
    "    r'\\(\\w+\\s+et\\s+al\\.?[,\\s]*\\d{4}\\)',  # (Smith et al., 2020)\n",
    "    r'\\(\\w+[,\\s]+\\d{4}\\)',  # (Smith, 2020)\n",
    "    \n",
    "    # Слова-индикаторы установленного знания\n",
    "    r'\\b(previous studies|prior research|previous work|earlier studies|literature shows)\\b',\n",
    "    r'\\b(it is known|it has been shown|it is established|it is well-known|it is recognized)\\b',\n",
    "    r'\\b(studies have shown|research has shown|evidence suggests|findings indicate)\\b',\n",
    "    r'\\b(has been demonstrated|has been reported|has been observed|has been found)\\b',\n",
    "]\n",
    "\n",
    "# Слова-исключения (указывают на гипотезы/выводы, а не факты)\n",
    "hypothesis_indicators = [\n",
    "    r'\\b(we hypothesize|we propose|we suggest|we predict|we expect|we anticipate)\\b',\n",
    "    r'\\b(our study|this study|present study|current study)\\b',\n",
    "    r'\\b(aim|objective|goal)\\b',\n",
    "]\n",
    "\n",
    "compiled_fact_patterns = [re.compile(p, re.IGNORECASE) for p in fact_patterns]\n",
    "compiled_hypothesis_patterns = [re.compile(p, re.IGNORECASE) for p in hypothesis_indicators]"
   ],
   "id": "eb1861e6615bf4cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T07:07:21.117491Z",
     "start_time": "2025-10-12T07:07:20.599235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "facts = []\n",
    "doc = nlp(parsed_doc.text)\n",
    "entity_id = 0\n",
    "count = 0\n",
    "for sent in doc.sents:\n",
    "        print(count)\n",
    "        for ent in sent.ents:\n",
    "            print(ent.label_)\n",
    "        count += 1\n",
    "        print(sent.text)"
   ],
   "id": "20279948de0aa32a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "ORG\n",
      "ORG\n",
      "ORG\n",
      "PERSON\n",
      "ORG\n",
      "ORG\n",
      "CARDINAL\n",
      "DATE\n",
      "GPE\n",
      "GPE\n",
      "CARDINAL\n",
      "GPE\n",
      "GPE\n",
      "LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows ⋆ Samy Ateia1 , Udo Kruschwitz1 , Melanie Scholz2, Agnes Koschmider2 , and Moayad Almohaishi2 1 University of Regensburg, Universitätsstraße 31, 93053 Regensburg, Germany {udo.kruschwitz,samy.ateia}@ur.de 2 University of Bayreuth, Universitätsstraße 30, 95447 Bayreuth, Germany {melanie.scholz,agnes.koschmider,moayad.almohaishi}@uni-bayreuth.de Abstract.\n",
      "1\n",
      "The increasing volume of scholarly publications requires ad- vanced tools for efficient knowledge discovery and management.\n",
      "2\n",
      "PERSON\n",
      "This paper introduces ongoing work on a system using Large Language Mod- els (LLMs) for the semantic extraction of key concepts from scientific documents.\n",
      "3\n",
      "NORP\n",
      "ORG\n",
      "ORG\n",
      "ORG\n",
      "Our research, conducted within the German National Re- search Data Infrastructure for and with Computer Science (NFDIx CS) project, seeks to support FAIR (Findable, Accessible, Interoperable, and Reusable) principles in scientific publishing.\n",
      "4\n",
      "ORG\n",
      "ORG\n",
      "We outline our explorative work, which uses in-context learning with various LLMs to extract con- cepts from papers, initially focusing on the Business Process Manage- ment (BPM) domain.\n",
      "5\n",
      "CARDINAL\n",
      "A key advantage of this approach is its potential for rapid domain adaptation, often requiring few or even zero examples to define extraction targets for new scientific fields.\n",
      "6\n",
      "We conducted technical evaluations to compare the performance of commercial and open-source LLMs and created an online demo application to collect feedback from an initial user-study.\n",
      "7\n",
      "Additionally, we gathered insights from the com- puter science research community through user stories collected during a dedicated workshop, actively guiding the ongoing development of our future services.\n",
      "8\n",
      "These services aim to support structured literature re- views, concept-based information retrieval, and integration of extracted knowledge into existing knowledge graphs.\n",
      "9\n",
      "ORG\n",
      "PERSON\n",
      "Keywords: Large Language Models · Information Extraction · Scien- tific Publishing · Digital Libraries · FAIR Principles · Knowledge Graphs.\n",
      "10\n",
      "DATE\n",
      "ORG\n",
      "ORG\n",
      "ORG\n",
      "DATE\n",
      "⋆Published in TPDL 2025, New Trends in Theory and Practice of Digital Libraries, Communications in Computer and Information Science, vol 2694.\n",
      "11\n",
      "CARDINAL\n",
      "DOI 10.1007/978- 3-032-06136-2_9.\n",
      "12\n",
      "ORG\n",
      "This PDF is the author-prepared camera-ready version corre- sponding to the accepted manuscript and supersedes the submitted version that was inadvertently published as the version of record.\n",
      "13\n",
      "PERSON\n",
      "ar Xiv:2510.04749v1\n",
      "14\n",
      "CARDINAL\n",
      "PERCENT\n",
      "ORG\n",
      "CARDINAL\n",
      "[cs.DL] 6 Oct 2025 2 S. Ateia et al. 1 Introduction The scientific publication landscape is booming with global annual publication growing by 59% according to the NSF3 and more than one million articles be-\n",
      "15\n",
      "CARDINAL\n",
      "ing published per year in biomedicine and life sciences alone [6].\n",
      "16\n",
      "This rise in publications makes it harder for scientists to stay on top of their field, while also limiting the discoverability of their publications as they have to compete with others for visibility.\n",
      "17\n",
      "Digital transformation and new tools such as LLMs can accelerate that problem, but also have the potential to assist scientists and publishing platforms in managing these challenges.\n",
      "18\n",
      "ORG\n",
      "In computer science, publications are often accompanied by software arti- facts and datasets for reproducibility, but their management frequently lacks standardization and fails to meet FAIR principles.\n",
      "19\n",
      "ORG\n",
      "ORG\n",
      "PERSON\n",
      "ORG\n",
      "CARDINAL\n",
      "ORG\n",
      "GPE\n",
      "The National Research Data Infrastructure for and with Computer Science (NFDIx CS) project4 addresses this by creating an infrastructure to implement FAIR principles [21] for CS research outputs in Germany\n",
      "20\n",
      "CARDINAL\n",
      "[5].\n",
      "21\n",
      "But to make these artifacts findable it is necessary to link them to relevant semantic information from the publication text itself, for example, research questions or methods so that they are discoverably as related work by other scientists.\n",
      "22\n",
      "Our project, situated within the NFDIx CS initiative, aims to develop tools to exactly address this problem.\n",
      "23\n",
      "ORG\n",
      "We leverage Large Language Models (LLMs) for the semantic analysis of scientific text, with the goal of enhancing the FAIR principles for scholarly literature.\n",
      "24\n",
      "Specifically, we aim to: – Develop robust methods for automatically extracting key semantic concepts (e.g., research questions, methodologies, findings) from scientific papers.\n",
      "25\n",
      "– Explore mechanisms for structuring these extracted concepts to improve the organization and distribution of digital content, potentially linking them to knowledge graphs.\n",
      "26\n",
      "– Design and prototype services, informed by community needs, that use this structured information to support researchers in their workflows.\n",
      "27\n",
      "This short paper presents our preliminary findings and outlines how user-driven requirements are shaping the trajectory of our research towards practical appli- cations.\n",
      "28\n",
      "We publish a demo system alongside its source code under a permissive license5 alongside the results of our user workshops, and plan to maintain this practice for future services.\n",
      "29\n",
      "CARDINAL\n",
      "2 Related Work In this work, we explore the use of LLMs for knowledge extraction to improve scientific workflows within digital libraries and beyond.\n",
      "30\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "We review related efforts 3 https://web.archive.org/web/20250507134337/https://www.ncses.nsf.gov/ pubs/nsb202333/executive-summary 4 https://nfdixcs.org/ 5 CC BY 4.0 LLM Info Extraction for Research Workflows 3 in three key areas: platforms for scientific literature analysis, knowledge graphs for structuring scientific information, and the use of LLMs for information ex- traction.\n",
      "31\n",
      "CARDINAL\n",
      "ORG\n",
      "2.1 Platforms for Scientific Literature Analysis Several platforms exist that use natural language processing (NLP) based on language models to highlight relevant information from scientific text, there- fore assisting in navigating the considerable volume of publications.\n",
      "32\n",
      "ORG\n",
      "Semantic Scholar\n",
      "33\n",
      "CARDINAL\n",
      "GPE\n",
      "ORG\n",
      "PERSON\n",
      "[2], for example, employs AI to provide summaries (TLDRs) and iden- tify influential citations, while Elicit uses a systematic review inspired work- flow, leveraging LLMs to synthesize findings from multiple papers in response to a user’s query\n",
      "34\n",
      "CARDINAL\n",
      "[20].\n",
      "35\n",
      "CARDINAL\n",
      "Scite.ai focuses specifically on citation context, classifying whether a citation supports, disputes, or merely mentions a claim [9].\n",
      "36\n",
      "ORG\n",
      "While these platforms offer similar flexible LLM-based question answering tools, they do not currently offer the use of predefined domain-specific questions and mostly require a paid subscription to be fully utilized.\n",
      "37\n",
      "With our approach, we want to potentially offer higher accuracy and user guidance through curated extraction targets and examples serving specialized communities.\n",
      "38\n",
      "CARDINAL\n",
      "2.2 Knowledge Graphs for Structuring Scientific Knowledge Structuring scientific knowledge in a machine-readable format has long been a goal of the scientific community.\n",
      "39\n",
      "ORG\n",
      "The Open Research Knowledge Graph (ORKG)\n",
      "40\n",
      "CARDINAL\n",
      "[8] is a prominent initiative aiming to represent the content of research papers as structured data.\n",
      "41\n",
      "ORG\n",
      "By describing papers through their contributions, methods, and findings, the ORKG facilitates systematic comparisons and reviews.\n",
      "42\n",
      "ORG\n",
      "ORG\n",
      "Other notable examples are the discontinued Microsoft Academic Graph (MAG)\n",
      "43\n",
      "CARDINAL\n",
      "PRODUCT\n",
      "CARDINAL\n",
      "PERSON\n",
      "[18] that was succeeded by Open Alex [13] or Sci KGraph\n",
      "44\n",
      "CARDINAL\n",
      "[17].\n",
      "45\n",
      "However, curating such knowledge graphs often requires significant manual effort from researchers.\n",
      "46\n",
      "ORG\n",
      "CARDINAL\n",
      "Most recently, ORKG Ask was introduced, which offers the possibility to create ad-hoc comparison tables using information extraction with LLMs [11].\n",
      "47\n",
      "In our work, we want to build on that approach and take it a step further.\n",
      "48\n",
      "Instead of just having users query questions on a set of retrieved papers, we explore how curated questions from domain-experts can be leveraged to prefill knowledge graph input templates for users.\n",
      "49\n",
      "ORG\n",
      "This complements the KG vision by lowering the barrier to entry and scaling up content acquisition.\n",
      "50\n",
      "Embedding and indexing the extracted information in separate fields could lead to improved semantic search, by enabling users to search for papers with similar research questions or algorithms.\n",
      "51\n",
      "CARDINAL\n",
      "PERSON\n",
      "2.3 LLMs for Information Extraction in Science In-context learning with LLMs describes the ability of these models to solve problems that they have not explicitly been trained on, by just giving the model 4 S. Ateia et al.\n",
      "52\n",
      "an abstract description of the problem (Zero-Shot) or several examples (Few- Shot) in their input context.\n",
      "53\n",
      "ORDINAL\n",
      "CARDINAL\n",
      "These approaches were first popularized with LLMs like GPT-3 [3] and enable their use in domains where limited, or no training data is available.\n",
      "54\n",
      "ORG\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "Recent LLMs such as the Google Gemini series6 or Open AIs GPT- 4.17 have pushed the size of the available context up to 1 million input tokens.\n",
      "55\n",
      "Which makes it feasible to extract information from large text sources in a single step.\n",
      "56\n",
      "These properties can be used in retrieval augmented generation (RAG)\n",
      "57\n",
      "CARDINAL\n",
      "[16] systems that ground the knowledge of these models in relevant texts.\n",
      "58\n",
      "ORG\n",
      "Systems such as CORE-GPT have shown the usefulness of such approaches in questions answering across multiple scientific domains\n",
      "59\n",
      "CARDINAL\n",
      "[12].\n",
      "60\n",
      "In our work, we explore both zero- and few-shot learning for extracting pre- defined semantic information from scientific texts, that can then be used to facilitate scientific knowledge discovery and publication workflows.\n",
      "61\n",
      "CARDINAL\n",
      "ORG\n",
      "3 Methodology: LLM-Based Concept Extraction Our system uses an LLM to extract semantic information from scientific doc- uments.\n",
      "62\n",
      "ORG\n",
      "The demo UI allows a user to upload a paper and pose predefined or custom questions.\n",
      "63\n",
      "ORG\n",
      "CARDINAL\n",
      "The LLM then processes the document and a prompt to iden- tify and return relevant information or synthesized answers (Figure 1).\n",
      "64\n",
      "PERSON\n",
      "Fig.\n",
      "65\n",
      "CARDINAL\n",
      "1. LLM-based demo extraction pipeline.\n",
      "66\n",
      "CARDINAL\n",
      "3.1\n",
      "67\n",
      "ORG\n",
      "In-context Learning for Rapid Domain Adaptation We leverage the in-context learning of modern LLMs for rapid domain adap- tation.\n",
      "68\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "ORG\n",
      "CARDINAL\n",
      "By providing instructions and a few examples to guide extraction, our 6 https://web.archive.org/web/20250607225206/https://blog.google/ technology/ai/google-gemini-next-generation-model-february-2024/ 7 https://web.archive.org/web/20250612080402/https://openai.com/index/ gpt-4-1/ LLM Info Extraction for Research Workflows 5 method avoids the need for the extensive, domain-specific datasets required by traditional supervised techniques.\n",
      "69\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "In our evaluations we tested two modes: A few-shot mode where we supplied three in-domain examples each consisting of 1.\n",
      "70\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "the full text of the document; 2. the domain-specific information extraction questions, 3. the instructions, 4. the manually crafted ideal answer from the document.\n",
      "71\n",
      "ORG\n",
      "In addition, we tested a zero- shot mode where we only supplied the instruction and the full text of the PDF.\n",
      "72\n",
      "CARDINAL\n",
      "The zero-shot mode formed our baseline for evaluation, but could also be used to offer the flexibility to the user to pose their own extraction questions against a document or a set of retrieved documents.\n",
      "73\n",
      "The few-shot mode aligns the model with the style of the manual annotators, overcoming limited or missing instructions in the way the question was posed.\n",
      "74\n",
      "This mode could be used in set- tings where predefined questions and predictable answer formats are important, for example when prefilling forms for later knowledge graph mapping.\n",
      "75\n",
      "CARDINAL\n",
      "PERSON\n",
      "CARDINAL\n",
      "WORK_OF_ART\n",
      "We tested four different models: Qwen 2.5 72B instruct [14], Llama 3.3 70B instruct8\n",
      "76\n",
      "CARDINAL\n",
      "PERSON\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "PERSON\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "[7], Gemini 1.5 Flash 002, Gemini 1.5 Flash 8B 001\n",
      "77\n",
      "CARDINAL\n",
      "[4].\n",
      "78\n",
      "PERSON\n",
      "ORG\n",
      "PRODUCT\n",
      "Llama and Qwen were accessed via openrouter.ai while the Gemini models were accessed via the official Google API.\n",
      "79\n",
      "CARDINAL\n",
      "The instructions used chain of thought prompting [19] to generate a reason- ing beside relevant context and the final answer.\n",
      "80\n",
      "CARDINAL\n",
      "GPE\n",
      "CARDINAL\n",
      "The exact zero-shot prompt can be seen in Listing 1.1.\n",
      "81\n",
      "CARDINAL\n",
      "Listing 1.1.\n",
      "82\n",
      "CARDINAL\n",
      "ORG\n",
      "CARDINAL\n",
      "Zero-shot prompt example in Python Extract the information answering the following question from the text: Question: ‘‘‘{question}‘‘‘ Text: ‘‘‘{text}‘‘‘ Return a JSON object in the following format: {{ \"reasoning \": \"<think step by step and write down your reasoning >\", \"context \": \"<contains all relevant context from the text >\", \"answer \": \"<one concise answer to the question for example: yes/no/none , or a word or multiple words >\" }} Try to be concise and limit your reasoning , answer , and the extracted context to max 500 words.\n",
      "83\n",
      "CARDINAL\n",
      "ORG\n",
      "GPE\n",
      "CARDINAL\n",
      "ORG\n",
      "ORG\n",
      "3.2 Dataset and Domain The initial development and a preliminary evaluation were carried out on a corpus of 122 scientific papers from the Business Process Management (BPM) conferences (2019–2023)9.\n",
      "84\n",
      "This domain was selected due to the availability of domain experts who are actively constructing a knowledge graph in this area.\n",
      "85\n",
      "CARDINAL\n",
      "ORG\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "8 https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/ 9 https://bpm-conference.org/conferences/ 6 S. Ateia et al.\n",
      "86\n",
      "Key concepts were manually annotated in the papers to establish a gold standard for evaluating extraction performance.\n",
      "87\n",
      "CARDINAL\n",
      "ORG\n",
      "ORG\n",
      "3.3 Extraction Example An example of the extraction process for the Target Concept of a \"Research Question\" would involve posing the Query: \"What is the explicitly stated re- search question for the paper?\".\n",
      "88\n",
      "The system might then return an Example Extracted Answer like: \"How to decide which processes need to be analyzed in detail to determine if changes are necessary.\"\n",
      "89\n",
      "This methodology is highly flexible, allowing us to target a wide array of semantic information within scientific texts with high adaptability, as only lim- ited domain expert involvement is needed to create a few examples for each information item.\n",
      "90\n",
      "CARDINAL\n",
      "ORG\n",
      "PERSON\n",
      "3.4 Demo System To showcase the ability of the tool and collect initial user feedback, we built a demo UI using the Gradio framework[1] around our approach.\n",
      "91\n",
      "PERSON\n",
      "The demo system is available online10(user:demo, pw:demo) and the source code for this system is available on Git Hub11.\n",
      "92\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "3.5 User Feedback In a pilot user study, we collected initial user-feedback with the demo system through a questionnaire after instructing a panel of users to choose one paper from a selection of business processing domain papers, upload it to the tool and select any questions that they were interested in.\n",
      "93\n",
      "CARDINAL\n",
      "At a separate workshop with around 30 participants from different computer- science fields, we collected user-stories that they would like to be solved by the offered and demonstrated technology.\n",
      "94\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "4 Results We evaluated model performance against a gold-standard dataset of 122 manu- ally annotated papers.\n",
      "95\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "We used three metrics based on the target type: 1.\n",
      "96\n",
      "CARDINAL\n",
      "DATE\n",
      "ORG\n",
      "ORG\n",
      "CARDINAL\n",
      "For categorical targets (15 targets, 1121 annotations), we used Exact Acc, a token-set accuracy based on a Jaccard similarity with a threshold of 0.8 [10,15].\n",
      "97\n",
      "CARDINAL\n",
      "2.\n",
      "98\n",
      "CARDINAL\n",
      "QUANTITY\n",
      "GPE\n",
      "PERSON\n",
      "For binary targets (13 targets, 984 annotations), we used the macro-averaged F1-score (Bin F1).\n",
      "99\n",
      "CARDINAL\n",
      "ORG\n",
      "DATE\n",
      "10 https://demo-d3.nfdixcs.org/ 11 https://github.com/Samy Ateia/nfdixcs-d3-knowledge-extraction-demo LLM Info Extraction for Research Workflows 7 3.\n",
      "100\n",
      "CARDINAL\n",
      "QUANTITY\n",
      "GPE\n",
      "GPE\n",
      "For free-text targets (4 targets, 488 annotations), we assessed semantic equiv- alence with the F1-score from BERTScore\n",
      "101\n",
      "CARDINAL\n",
      "[22].\n",
      "102\n",
      "LAW\n",
      "CARDINAL\n",
      "The Overall score in Table 1 is the unweighted arithmetic mean of these three metrics.\n",
      "103\n",
      "CARDINAL\n",
      "Table 1.\n",
      "104\n",
      "Model comparison on the paper-coding benchmark (higher = better)\n",
      "105\n",
      "PRODUCT\n",
      "Model Exact Acc Bin F1 BERT_F1\n",
      "106\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "WORK_OF_ART\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "PRODUCT\n",
      "CARDINAL\n",
      "DATE\n",
      "MONEY\n",
      "CARDINAL\n",
      "PRODUCT\n",
      "CARDINAL\n",
      "MONEY\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "PERSON\n",
      "CARDINAL\n",
      "ORG\n",
      "Overall qwen-2.5-72b (3-shot) 0.249 0.594 0.863 0.569 qwen-2.5-72b (0-shot) 0.219 0.514 0.887 0.540 llama-3.3-70b (0-shot) 0.212 0.556 0.877 0.548 gemini-1.5-flash-002 (3-shot) 0.246 0.330 0.893 0.490 gemini-1.5-flash-002 (0-shot) 0.183 0.390 0.883 0.486 gemini-1.5-flash-8b-001 (0-shot) 0.171 0.345 0.881 0.466 gemini-1.5-flash-8b-001 (3-shot) 0.180 0.148 0.897 0.408 BERT_F1 scores near 0.90 indicate strong semantic alignment on free-text fields, whereas binary indicators show moderate performance (best Bin F1 = 0.59) and exact categorical extraction remains limited (Exact Acc < 0.25).\n",
      "107\n",
      "CARDINAL\n",
      "ORG\n",
      "ORG\n",
      "PERCENT\n",
      "4.1 User Study & Workshop Feedback from our pilot user study on the prototype demo UI was positive (88% satisfaction with extracted concepts), indicating the potential utility of the approach.\n",
      "108\n",
      "ORG\n",
      "While some feedback was UI related (hiding advanced configuration like few-shot examples, wanting more expert configuration), a main point was that the traceability of the extracted information should be improved.\n",
      "109\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "In a separate workshop with around 30 computer-science researchers, we col- lected 56 user-stories.\n",
      "110\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "38 of these focused on the task of literature research and comparison, 8 on assistance while writing papers, 3 on support in the review process, 3 were directed towards software development and 4 were unique.\n",
      "111\n",
      "Over- all, it became clear that the users want to go beyond just extracting concepts from one specific paper and compare the extracted information from multiple papers instead.\n",
      "112\n",
      "The full categorized list is available in our repository12.\n",
      "113\n",
      "CARDINAL\n",
      "5 Discussion The results of our technical evaluation and user-studies, while preliminary, pro- vide valuable direction for the development of our future services.\n",
      "114\n",
      "CARDINAL\n",
      "ORG\n",
      "PERSON\n",
      "GPE\n",
      "Few-shot examples seemed to improve the performance of the models in tasks where specific categorial answers were needed and on the free-text extractions 12 https://github.com/Samy Ateia/nfdixcs-d3-knowledge-extraction-demo 8 S. Ateia et al. measured by BERTScore.\n",
      "115\n",
      "But on the binary classification task, the performance decreased.\n",
      "116\n",
      "This could be explained by a class bias introduced via the few-shot examples, while on the textual extractions the examples might have informed the model better about the expected format of the answers.\n",
      "117\n",
      "Using the full-text of documents in few-shot examples is costly and poten- tially increases noise.\n",
      "118\n",
      "We are working on exploring the impact of more and shorter examples and selecting ideal examples for specific extraction target types.\n",
      "119\n",
      "PERSON\n",
      "WORK_OF_ART\n",
      "The open-weight models Qwen 2.5-72B-Instruct and Llama 3.3-70B-Instruct seemed to perform better than the commercial models that we tested.\n",
      "120\n",
      "ORG\n",
      "For the Gemini 1.5-flash-8b model, this is most likely explained by the difference in size.\n",
      "121\n",
      "PERSON\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "The size of the normal Gemini 1.5-flash model is unknown but given our results and the cost and speed we suspect that it is also smaller than the 70 billion parameter models that we compared them to.\n",
      "122\n",
      "From the feedback that we collected through our pilot user study and the discussion in a later workshop, it became clear that there is a need for better transparency and traceability.\n",
      "123\n",
      "Ideally, highlighting the text passages that inform an extracted information items in the source document.\n",
      "124\n",
      "Even though there are commercial services available that are similar to our tool, our contribution can inform researchers and professionals that want to offer customizable domain-specific services to their users.\n",
      "125\n",
      "We demonstrate the feasibility of in-context learning and open-weights models for these use-cases and publish our code to boost independent development of transparent services.\n",
      "126\n",
      "CARDINAL\n",
      "6 Conclusion and Ongoing Work We confirmed the potential of current LLMs to summarize and extract domain- specific information from scientific text.\n",
      "127\n",
      "Through in-context learning, these mod- els can be quickly adapted to specific scientific domains and facilitate the transfer of expert knowledge between researchers by highlighting and comparing key as- pects of their work.\n",
      "128\n",
      "Through our user-centric approach, we collected valuable feedback and user- stories that can guide the development of current and future services.\n",
      "129\n",
      "ORG\n",
      "Notable transparency and the need that services enable the user to verify LLM generated output by tracing summarized information back to the source text.\n",
      "130\n",
      "DATE\n",
      "Our ongoing work will focus on exploring embedding-based retrieval on the extracted structured information, therefore overcoming the arbitrary chunking issue that limits semantic relevance in vector search.\n",
      "131\n",
      "ORG\n",
      "We’re also exploring how our approach can be integrated in the publishing process, prefilling templates for knowledge graph mapping e.g., for ORKG.\n",
      "132\n",
      "Making it easier for authors to fill out forms that facilitate the discoverability of their work.\n",
      "133\n",
      "Overall, our work highlights the potential of LLMs to improve the publishing process and discoverability of scientific information in digital libraries and be- yond.\n",
      "134\n",
      "Its main contribution is demonstrating the practical integration of these models into a user-focused, open-source system designed to tackle real-world challenges, rather than proposing a novel extraction algorithm itself.\n",
      "135\n",
      "ORG\n",
      "CARDINAL\n",
      "LLM Info Extraction for Research Workflows 9 Acknowledgments.\n",
      "136\n",
      "We thank the anonymous reviewers for their valuable feed- back.\n",
      "137\n",
      "ORG\n",
      "ORG\n",
      "DATE\n",
      "This work is funded by the German Research Foundation (DFG) as part of the NFDIx CS consortium (Grant number: 501930651).\n",
      "138\n",
      "NORP\n",
      "Disclosure of Interests.\n",
      "139\n",
      "The authors have no competing interests to declare that are relevant to the content of this article.\n",
      "140\n",
      "CARDINAL\n",
      "References 1.\n",
      "141\n",
      "GPE\n",
      "GPE\n",
      "NORP\n",
      "GPE\n",
      "PERSON\n",
      "PERSON\n",
      "DATE\n",
      "CARDINAL\n",
      "Abid, A., Abdalla, A., Abid, A., Khan, D., Alfozan, A., Zou, J.: Gradio: Hassle- free sharing and testing of ml models in the wild (2019), https://arxiv.org/abs/ 1906.02569 2.\n",
      "142\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "NORP\n",
      "GPE\n",
      "ORG\n",
      "GPE\n",
      "NORP\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "Ammar, W., Groeneveld, D., Bhagavatula, C., Beltagy, I., Crawford, M., Downey, D., Dunkelberger, J., Elgohary, A., Feldman, S., Ha, V., et al.:\n",
      "143\n",
      "Construction of the literature graph in semantic scholar.\n",
      "144\n",
      "PERSON\n",
      "DATE\n",
      "CARDINAL\n",
      "ar Xiv preprint ar Xiv:1805.02262 (2018) 3.\n",
      "145\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "ORG\n",
      "ORG\n",
      "GPE\n",
      "GPE\n",
      "GPE\n",
      "ORG\n",
      "PRODUCT\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee- lakantan, A., Shyam, P., Sastry, G., Askell, A., et al.:\n",
      "146\n",
      "Language models are few-shot learners.\n",
      "147\n",
      "CARDINAL\n",
      "DATE\n",
      "DATE\n",
      "CARDINAL\n",
      "Advances in neural information processing systems 33, 1877–1901 (2020) 4.\n",
      "148\n",
      "PERSON\n",
      "PERSON\n",
      "CARDINAL\n",
      "DATE\n",
      "Georgiev, P., et al.: Gemini 1.5: Unlocking multimodal understanding across mil- lions of tokens of context (2024), https://arxiv.org/abs/2403.05530 5.\n",
      "149\n",
      "ORG\n",
      "PERSON\n",
      "Goedicke, M., Lucke, U.: Research Data Management in Computer Science- NFDIx CS Approach.\n",
      "150\n",
      "DATE\n",
      "In: INFORMATIK 2022.\n",
      "151\n",
      "PERSON\n",
      "pp.\n",
      "152\n",
      "CARDINAL\n",
      "1317–1328.\n",
      "153\n",
      "GPE\n",
      "DATE\n",
      "CARDINAL\n",
      "Gesellschaft für In- formatik, Bonn (2022) 6.\n",
      "154\n",
      "ORG\n",
      "NORP\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "GPE\n",
      "PERSON\n",
      "PERSON\n",
      "NORP\n",
      "González-Márquez, R., Schmidt, L., Schmidt, B.M., Berens, P., Kobak, D.: The landscape of biomedical research.\n",
      "155\n",
      "CARDINAL\n",
      "DATE\n",
      "DATE\n",
      "Patterns 5(6), 100968 (2024).\n",
      "156\n",
      "PERSON\n",
      "https://doi.org/ https://doi.org/10.1016/j.patter.2024.100968 7.\n",
      "157\n",
      "DATE\n",
      "PERSON\n",
      "Grattafiori, A., et al.:\n",
      "158\n",
      "DATE\n",
      "CARDINAL\n",
      "The Llama 3 Herd of Models (2024), https://arxiv.org/ abs/2407.21783 8.\n",
      "159\n",
      "PERSON\n",
      "GPE\n",
      "ORG\n",
      "PERSON\n",
      "PERSON\n",
      "Jaradeh, M.Y., Oelen, A., Prinz, M., Stocker, M., Auer, S.: Open research knowl- edge graph: a system walkthrough.\n",
      "160\n",
      "ORG\n",
      "ORDINAL\n",
      "ORG\n",
      "DATE\n",
      "GPE\n",
      "GPE\n",
      "DATE\n",
      "CARDINAL\n",
      "In: Digital Libraries for Open Knowledge: 23rd International Conference on Theory and Practice of Digital Libraries, TPDL 2019, Oslo, Norway, September 9-12, 2019, Proceedings 23.\n",
      "161\n",
      "pp.\n",
      "162\n",
      "CARDINAL\n",
      "348–351.\n",
      "163\n",
      "DATE\n",
      "CARDINAL\n",
      "Springer (2019) 9.\n",
      "164\n",
      "PERSON\n",
      "PERSON\n",
      "GPE\n",
      "ORG\n",
      "Lund, B., Shamsi, A.: Examining the use of supportive and contrasting citations in different disciplines: a brief study using Scite (scite.\n",
      "165\n",
      "ai) data.\n",
      "166\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "DATE\n",
      "CARDINAL\n",
      "Scientometrics 128(8), 4895–4900 (2023) 10.\n",
      "167\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "Mann, W., Augsten, N., Bouros, P.: An empirical evaluation of set similarity join techniques.\n",
      "168\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "Proceedings of the VLDB Endowment 9(9), 636–647 (2016) 11.\n",
      "169\n",
      "ORG\n",
      "PERSON\n",
      "GPE\n",
      "GPE\n",
      "ORG\n",
      "PERSON\n",
      "ORG\n",
      "Oelen, A., Jaradeh, M.Y., Auer, S.: ORKG ASK: A Neuro-symbolic Scholarly Search and Exploration System.\n",
      "170\n",
      "PERSON\n",
      "DATE\n",
      "CARDINAL\n",
      "ar Xiv preprint ar Xiv:2412.04977 (2024) 12.\n",
      "171\n",
      "NORP\n",
      "PERSON\n",
      "PERSON\n",
      "Pride, D., Cancellieri, M., Knoth, P.: CORE-GPT: Combining open access re- search and large language models for credible, trustworthy question answering.\n",
      "172\n",
      "ORG\n",
      "In: International Conference on Theory and Practice of Digital Libraries.\n",
      "173\n",
      "pp.\n",
      "174\n",
      "CARDINAL\n",
      "146–159.\n",
      "175\n",
      "DATE\n",
      "CARDINAL\n",
      "Springer (2023) 13.\n",
      "176\n",
      "PERSON\n",
      "GPE\n",
      "PERSON\n",
      "NORP\n",
      "Priem, J., Piwowar, H., Orr, R.: Open Alex: A fully-open index of scholarly works, authors, venues, institutions, and concepts.\n",
      "177\n",
      "PERSON\n",
      "DATE\n",
      "DATE\n",
      "ar Xiv preprint ar Xiv:2205.01833 (2022) 14.\n",
      "178\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "NORP\n",
      "NORP\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "GPE\n",
      "NORP\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "NORP\n",
      "PERSON\n",
      "PERSON\n",
      "NORP\n",
      "PERSON\n",
      "GPE\n",
      "NORP\n",
      "PERSON\n",
      "NORP\n",
      "GPE\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "GPE\n",
      "PERSON\n",
      "PERSON\n",
      "GPE\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "ORG\n",
      "DATE\n",
      "PERSON\n",
      "Qwen, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Tang, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, 10 S. Ateia et al. Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., Qiu, Z.: Qwen2.5 Technical Report (2025), https://arxiv.org/abs/2412.15115 15.\n",
      "179\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "ORG\n",
      "GPE\n",
      "NORP\n",
      "GPE\n",
      "GPE\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "ORG\n",
      "Schmidt, L., Mutlu, A.N.F., Elmore, R., Olorisade, B.K., Thomas, J., Higgins, J.P.:\n",
      "180\n",
      "Data extraction methods for systematic review (semi) automation: Update of a living systematic review.\n",
      "181\n",
      "CARDINAL\n",
      "DATE\n",
      "CARDINAL\n",
      "F1000Research 10, 401 (2025) 16.\n",
      "182\n",
      "NORP\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "GPE\n",
      "NORP\n",
      "GPE\n",
      "Shuster, K., Poff, S., Chen, M., Kiela, D., Weston, J.: Retrieval Augmentation Reduces Hallucination in Conversation.\n",
      "183\n",
      "DATE\n",
      "In: Findings of the Association for Com- putational Linguistics: EMNLP 2021.\n",
      "184\n",
      "PERSON\n",
      "pp.\n",
      "185\n",
      "DATE\n",
      "CARDINAL\n",
      "3784–3803 (2021) 17.\n",
      "186\n",
      "GPE\n",
      "PERSON\n",
      "GPE\n",
      "Tosi, M.D.L., dos Reis, J.C.:\n",
      "187\n",
      "PERSON\n",
      "Sci KGraph: A knowledge graph approach to structure a scientific field.\n",
      "188\n",
      "ORG\n",
      "DATE\n",
      "DATE\n",
      "Journal of Informetrics 15(1), 101109 (2021).\n",
      "189\n",
      "CARDINAL\n",
      "https://doi.org/https://doi.org/10.1016/j.joi.2020.101109, https://www.sciencedirect.com/science/article/pii/S175115772030626X 18.\n",
      "190\n",
      "PERSON\n",
      "NORP\n",
      "GPE\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "GPE\n",
      "GPE\n",
      "ORG\n",
      "ORG\n",
      "Wang, K., Shen, Z., Huang, C., Wu, C.H., Dong, Y., Kanakia, A.: Microsoft aca-\n",
      "191\n",
      "demic graph: When experts are not enough.\n",
      "192\n",
      "CARDINAL\n",
      "CARDINAL\n",
      "DATE\n",
      "CARDINAL\n",
      "Quantitative Science Studies 1(1), 396–413 (2020) 19.\n",
      "193\n",
      "PERSON\n",
      "PERSON\n",
      "ORG\n",
      "NORP\n",
      "NORP\n",
      "NORP\n",
      "GPE\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "GPE\n",
      "GPE\n",
      "GPE\n",
      "GPE\n",
      "PERSON\n",
      "NORP\n",
      "Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E.H., Le, Q.V., Zhou, D.: Chain-of-thought prompting elicits reasoning in large language models.\n",
      "194\n",
      "ORG\n",
      "In: Proceedings of the 36th International Conference on Neural Information Processing Systems.\n",
      "195\n",
      "CARDINAL\n",
      "ORG\n",
      "ORG\n",
      "GPE\n",
      "GPE\n",
      "DATE\n",
      "CARDINAL\n",
      "NIPS ’22, Curran Associates Inc., Red Hook, NY, USA (2022) 20.\n",
      "196\n",
      "PERSON\n",
      "PERSON\n",
      "GPE\n",
      "GPE\n",
      "Whitfield, S., Hofmann, M.A.:\n",
      "197\n",
      "Elicit: AI literature review research assistant.\n",
      "198\n",
      "DATE\n",
      "CARDINAL\n",
      "DATE\n",
      "DATE\n",
      "Public Services Quarterly 19(3), 201–207 (2023) 21.\n",
      "199\n",
      "GPE\n",
      "GPE\n",
      "ORG\n",
      "GPE\n",
      "GPE\n",
      "GPE\n",
      "PERSON\n",
      "GPE\n",
      "GPE\n",
      "PERSON\n",
      "PERSON\n",
      "GPE\n",
      "GPE\n",
      "GPE\n",
      "PERSON\n",
      "Wilkinson, M.D., Dumontier, M., Aalbersberg, I.J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.W., da Silva Santos, L.B., Bourne, P.E., et al.: The FAIR Guiding Principles for scientific data management and stewardship.\n",
      "200\n",
      "CARDINAL\n",
      "DATE\n",
      "DATE\n",
      "Scientific data 3(1), 1–9 (2016) 22.\n",
      "201\n",
      "PERSON\n",
      "GPE\n",
      "PERSON\n",
      "PERSON\n",
      "PERSON\n",
      "GPE\n",
      "PERSON\n",
      "Zhang, T., Kishore, V., Wu, F., Weinberger, K.Q., Artzi, Y.:\n",
      "202\n",
      "PERSON\n",
      "Bertscore: Evaluating text generation with bert.\n",
      "203\n",
      "PERSON\n",
      "DATE\n",
      "ar Xiv preprint ar Xiv:1904.09675 (2019)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_facts_from_text(text, section_name):\n",
    "    facts = []\n",
    "    doc = nlp(text)\n",
    "    entity_id = 0\n",
    "    for sent in doc.sents:\n",
    "        sent_text = sent.text.strip()\n",
    "        if len(sent_text.split()) < 5:\n",
    "            continue\n",
    "\n",
    "    return facts"
   ],
   "id": "79bb38f135e48a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Извлечение фактов из Introduction и Abstract\n",
    "print(\"Extracting facts...\\n\")\n",
    "\n",
    "all_facts = []\n",
    "\n",
    "# Приоритетные секции для фактов\n",
    "fact_sections = ['abstract', 'introduction', 'background']\n",
    "\n",
    "for section_name in fact_sections:\n",
    "    if section_name in parsed_doc.sections:\n",
    "        print(f\"Processing section: {section_name.upper()}\")\n",
    "        section_text = parsed_doc.sections[section_name]\n",
    "        \n",
    "        facts = extract_facts_from_text(section_text, section_name)\n",
    "        all_facts.extend(facts)\n",
    "        \n",
    "        print(f\"  Found {len(facts)} facts\\n\")\n",
    "\n",
    "print(f\"\\n✓ Total facts extracted: {len(all_facts)}\")"
   ],
   "id": "98f0734b5afa5594"
  },
  {
   "cell_type": "markdown",
   "id": "f2a3b4c5",
   "metadata": {},
   "source": [
    "## 4. Список извлеченных фактов"
   ]
  },
  {
   "cell_type": "code",
   "id": "a3b4c5d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T06:45:03.652569Z",
     "start_time": "2025-10-12T06:45:03.649031Z"
    }
   },
   "source": [
    "# Сортировка по уверенности\n",
    "all_facts.sort(key=lambda x: x.confidence, reverse=True)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"ИЗВЛЕЧЕННЫЕ ФАКТЫ\")\n",
    "print(\"=\" * 100)\n",
    "print()\n",
    "\n",
    "for i, fact in enumerate(all_facts, 1):\n",
    "    print(f\"[{i}] Confidence: {fact.confidence:.2f} | Section: {fact.source_section}\")\n",
    "    print(f\"    {fact.text}\")\n",
    "    print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ИЗВЛЕЧЕННЫЕ ФАКТЫ\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "b4c5d6e7",
   "metadata": {},
   "source": [
    "## 5. Экспорт фактов (опционально)"
   ]
  },
  {
   "cell_type": "code",
   "id": "c5d6e7f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T06:45:10.935145Z",
     "start_time": "2025-10-12T06:45:10.931446Z"
    }
   },
   "source": [
    "# Простой список фактов (только текст)\n",
    "facts_list = [fact.text for fact in all_facts]\n",
    "\n",
    "print(f\"Extracted {len(facts_list)} facts:\")\n",
    "print()\n",
    "for fact in facts_list:\n",
    "    print(f\"• {fact}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 0 facts:\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение в JSON (опционально)\n",
    "import json\n",
    "\n",
    "output_data = {\n",
    "    \"paper_id\": \"2510.04749v1\",\n",
    "    \"facts_count\": len(all_facts),\n",
    "    \"facts\": [fact.to_dict() for fact in all_facts]\n",
    "}\n",
    "\n",
    "output_path = project_root / \"results\" / \"spacy_facts_2510.04749v1.json\"\n",
    "output_path.parent.mkdir(exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✓ Facts saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
