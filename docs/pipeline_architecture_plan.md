# Pipeline Architecture Plan - Singularis Challenge

**–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 10 –æ–∫—Ç—è–±—Ä—è 2025
**–°—Ç–∞—Ç—É—Å:** Approved for Implementation
**–¶–µ–ª—å:** Cost-efficient –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π

---

## üéØ –°—Ç—Ä–∞—Ç–µ–≥–∏—è

### –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **–º–æ–¥—É–ª—å–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å –≤–∑–∞–∏–º–æ–∑–∞–º–µ–Ω—è–µ–º—ã–º–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞–º–∏**:

1. **LLM Pipeline (v1)** ‚Üí –°–æ–∑–¥–∞–Ω–∏–µ ground truth –¥–∞—Ç–∞—Å–µ—Ç–∞ (GPT-4, ~$0.30/—Å—Ç–∞—Ç—å—è)
2. **Regex Pipeline (v2)** ‚Üí –ü–∞—Ç—Ç–µ—Ä–Ω-based –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ (–±–µ—Å–ø–ª–∞—Ç–Ω–æ)
3. **Hybrid Pipeline (v3)** ‚Üí –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ (Regex + NLP + selective LLM, ~$0.02/—Å—Ç–∞—Ç—å—è)

### –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–∞

**–ü–æ—á–µ–º—É —Å–Ω–∞—á–∞–ª–∞ LLM?**
- –°–æ–∑–¥–∞–Ω–∏–µ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ ground truth –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
- –ï–¥–∏–Ω–æ—Ä–∞–∑–æ–≤—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ —Ä–∞–∑–º–µ—Ç–∫—É 10-15 —Å—Ç–∞—Ç–µ–π
- –û–±—ä–µ–∫—Ç–∏–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –¥–µ—à–µ–≤—ã—Ö –ø–∞–π–ø–ª–∞–π–Ω–æ–≤

**–ü–æ—á–µ–º—É –º–æ–¥—É–ª—å–Ω–æ—Å—Ç—å?**
- –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞–∑–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤
- –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
- –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (Bonus Points)

---

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          Streamlit Web Interface                 ‚îÇ
‚îÇ   [Upload] [Select Pipeline] [View Results]      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Pipeline Orchestrator                    ‚îÇ
‚îÇ  ‚Ä¢ Pipeline Registry                             ‚îÇ
‚îÇ  ‚Ä¢ Results Management                            ‚îÇ
‚îÇ  ‚Ä¢ Metrics Collection                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ           ‚îÇ          ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  LLM    ‚îÇ ‚îÇ Regex  ‚îÇ ‚îÇ Hybrid  ‚îÇ
    ‚îÇPipeline ‚îÇ ‚îÇPipeline‚îÇ ‚îÇPipeline ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ          ‚îÇ          ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  Validation Layer   ‚îÇ
         ‚îÇ  ‚Ä¢ Ground Truth DB  ‚îÇ
         ‚îÇ  ‚Ä¢ Metrics Computer ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ (–¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è)

### –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥—É–ª–∏ ‚úÖ
```
src/
‚îú‚îÄ‚îÄ llm_adapters/          # –ê–¥–∞–ø—Ç–µ—Ä—ã –¥–ª—è LLM (OpenAI, Ollama)
‚îú‚îÄ‚îÄ fetchers/              # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–µ–π (PubMed)
‚îú‚îÄ‚îÄ config/                # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
‚îî‚îÄ‚îÄ utils/                 # –£—Ç–∏–ª–∏—Ç—ã
```

### –ù–æ–≤—ã–µ –º–æ–¥—É–ª–∏ üÜï
```
src/
‚îú‚îÄ‚îÄ parsers/               # üÜï –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
‚îÇ   ‚îú‚îÄ‚îÄ base_parser.py           # –ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –∫–ª–∞—Å—Å
‚îÇ   ‚îú‚îÄ‚îÄ pdf_parser.py            # PDF ‚Üí text (PyMuPDF, pdfplumber)
‚îÇ   ‚îú‚îÄ‚îÄ txt_parser.py            # TXT ‚Üí text
‚îÇ   ‚îú‚îÄ‚îÄ html_parser.py           # HTML ‚Üí text (BeautifulSoup)
‚îÇ   ‚îî‚îÄ‚îÄ factory.py               # Parser factory
‚îÇ
‚îú‚îÄ‚îÄ pipelines/             # üÜï –ö–õ–Æ–ß–ï–í–û–ô –ú–û–î–£–õ–¨
‚îÇ   ‚îú‚îÄ‚îÄ base_pipeline.py         # –ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –∫–ª–∞—Å—Å Pipeline
‚îÇ   ‚îú‚îÄ‚îÄ llm_pipeline.py          # v1: GPT-4 –¥–ª—è ground truth
‚îÇ   ‚îú‚îÄ‚îÄ regex_pipeline.py        # v2: Regex-based
‚îÇ   ‚îú‚îÄ‚îÄ hybrid_pipeline.py       # v3: Hybrid (Regex + NLP + LLM)
‚îÇ   ‚îú‚îÄ‚îÄ registry.py              # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–æ–≤
‚îÇ   ‚îî‚îÄ‚îÄ orchestrator.py          # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞–º–∏
‚îÇ
‚îú‚îÄ‚îÄ extractors/            # üÜï –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
‚îÇ   ‚îú‚îÄ‚îÄ regex_extractor.py       # Regex –ø–∞—Ç—Ç–µ—Ä–Ω—ã
‚îÇ   ‚îú‚îÄ‚îÄ nlp_extractor.py         # spaCy/scispaCy NER
‚îÇ   ‚îú‚îÄ‚îÄ section_detector.py      # –î–µ—Ç–µ–∫—Ü–∏—è —Å–µ–∫—Ü–∏–π —Å—Ç–∞—Ç—å–∏
‚îÇ   ‚îî‚îÄ‚îÄ entity_linker.py         # –°–≤—è–∑–∏ –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏
‚îÇ
‚îú‚îÄ‚îÄ models/                # üÜï –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö
‚îÇ   ‚îú‚îÄ‚îÄ entities.py              # Fact, Hypothesis, Experiment, etc.
‚îÇ   ‚îú‚îÄ‚îÄ graph.py                 # Knowledge graph structure
‚îÇ   ‚îî‚îÄ‚îÄ results.py               # ExtractionResult, PipelineMetrics
‚îÇ
‚îú‚îÄ‚îÄ validation/            # üÜï –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –º–µ—Ç—Ä–∏–∫–∏
‚îÇ   ‚îú‚îÄ‚îÄ ground_truth.py          # Ground truth database
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py               # Precision, Recall, F1
‚îÇ   ‚îî‚îÄ‚îÄ comparator.py            # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–æ–≤
‚îÇ
‚îî‚îÄ‚îÄ storage/               # üÜï –•—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    ‚îú‚îÄ‚îÄ results_db.py            # SQLite –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    ‚îî‚îÄ‚îÄ chroma_store.py          # ChromaDB wrapper

ui/                        # üÜï Streamlit UI
‚îú‚îÄ‚îÄ app.py                       # Main application
‚îú‚îÄ‚îÄ pages/
‚îÇ   ‚îú‚îÄ‚îÄ 1_Upload_Papers.py
‚îÇ   ‚îú‚îÄ‚îÄ 2_Run_Pipeline.py
‚îÇ   ‚îú‚îÄ‚îÄ 3_View_Results.py
‚îÇ   ‚îî‚îÄ‚îÄ 4_Compare_Pipelines.py
‚îî‚îÄ‚îÄ components/
    ‚îú‚îÄ‚îÄ graph_viewer.py          # Knowledge graph visualization
    ‚îî‚îÄ‚îÄ metrics_dashboard.py     # Metrics display

ground_truth/              # üÜï –≠—Ç–∞–ª–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
‚îú‚îÄ‚îÄ papers/                      # –†–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
‚îÇ   ‚îú‚îÄ‚îÄ paper_001.json
‚îÇ   ‚îú‚îÄ‚îÄ paper_002.json
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ annotations.json             # Ground truth —Ä–∞–∑–º–µ—Ç–∫–∞
```

---

## üîß –î–µ—Ç–∞–ª—å–Ω–∞—è —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è –º–æ–¥—É–ª–µ–π

### 1. Base Pipeline Interface

**–§–∞–π–ª:** `src/pipelines/base_pipeline.py`

```python
from abc import ABC, abstractmethod
from typing import Dict, List, Any
from dataclasses import dataclass
from datetime import datetime

@dataclass
class ExtractionResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞–±–æ—Ç—ã –ø–∞–π–ø–ª–∞–π–Ω–∞"""
    paper_id: str
    entities: Dict[str, List[Any]]  # {"facts": [...], "hypotheses": [...], ...}
    relationships: List[Dict]        # [{"source": ..., "target": ..., "type": ...}]
    metadata: Dict[str, Any]         # –í—Ä–µ–º—è, —Å—Ç–æ–∏–º–æ—Å—Ç—å, —Ç–æ–∫–µ–Ω—ã
    timestamp: datetime

@dataclass
class PipelineMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    processing_time: float
    tokens_used: int
    cost_usd: float
    entities_extracted: int
    memory_used_mb: float


class BasePipeline(ABC):
    """–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –ø–∞–π–ø–ª–∞–π–Ω–æ–≤"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.name = self.__class__.__name__
        self.last_metrics = None

    @abstractmethod
    def extract(self, paper_text: str, paper_id: str) -> ExtractionResult:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ —Å—Ç–∞—Ç—å–∏

        Args:
            paper_text: –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
            paper_id: –£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å—Ç–∞—Ç—å–∏

        Returns:
            ExtractionResult —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏ —Å—É—â–Ω–æ—Å—Ç—è–º–∏
        """
        pass

    @abstractmethod
    def get_metrics(self) -> PipelineMetrics:
        """–ü–æ–ª—É—á–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞"""
        pass

    @property
    @abstractmethod
    def description(self) -> str:
        """–û–ø–∏—Å–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è UI"""
        pass

    @property
    @abstractmethod
    def estimated_cost_per_paper(self) -> float:
        """–û—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–æ—á–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –Ω–∞ —Å—Ç–∞—Ç—å—é –≤ USD"""
        pass

    @property
    def version(self) -> str:
        """–í–µ—Ä—Å–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        return "1.0.0"
```

---

### 2. LLM Pipeline (Ground Truth Generation)

**–§–∞–π–ª:** `src/pipelines/llm_pipeline.py`

**–¶–µ–ª—å:** –°–æ–∑–¥–∞–Ω–∏–µ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ ground truth –¥–∞—Ç–∞—Å–µ—Ç–∞

**–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:**
- –ú–æ–¥–µ–ª—å: GPT-4 –∏–ª–∏ GPT-4o (–≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å)
- –°—Ç–æ–∏–º–æ—Å—Ç—å: ~$0.30 –Ω–∞ —Å—Ç–∞—Ç—å—é
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: –†–∞–∑–æ–≤–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ 10-15 —Å—Ç–∞—Ç–µ–π
- Precision: ~95% (–æ–∂–∏–¥–∞–µ–º–æ–µ)

**–ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
```python
class LLMPipeline(BasePipeline):
    def __init__(self, config):
        super().__init__(config)
        self.llm = get_llm_adapter("openai")
        self.model = "gpt-4o"  # –ë–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å
        self.temperature = 0.1  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏–∑–º–∞

    def extract(self, paper_text: str, paper_id: str):
        # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
        prompt = self._build_extraction_prompt(paper_text)

        # JSON-—Ä–µ–∂–∏–º –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞
        response = self.llm.generate(
            prompt=prompt,
            response_format={"type": "json_object"}
        )

        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –ø–∞—Ä—Å–∏–Ω–≥
        entities = self._parse_and_validate(response)

        return ExtractionResult(...)
```

**–ü—Ä–æ–º–ø—Ç-—Å—Ç—Ä–∞—Ç–µ–≥–∏—è:**
- Few-shot examples (3-5 –ø—Ä–∏–º–µ—Ä–æ–≤)
- –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π JSON output
- –Ø–≤–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ —Å—É—â–Ω–æ—Å—Ç–∏
- –ü—Ä–∏–º–µ—Ä—ã —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏

---

### 3. Regex Pipeline (Cost-Free Baseline)

**–§–∞–π–ª:** `src/pipelines/regex_pipeline.py`

**–¶–µ–ª—å:** –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –¥–µ—à–µ–≤–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤

**–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:**
- –°—Ç–æ–∏–º–æ—Å—Ç—å: $0 (—Ç–æ–ª—å–∫–æ CPU)
- –°–∫–æ—Ä–æ—Å—Ç—å: ~200-300 —Å—Ç–∞—Ç–µ–π/—á–∞—Å
- Precision: ~60-70% (–æ–∂–∏–¥–∞–µ–º–æ–µ)
- Recall: ~50-60% (–æ–∂–∏–¥–∞–µ–º–æ–µ)

**–ò–∑–≤–ª–µ–∫–∞–µ–º—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã:**
```python
# –ü—Ä–∏–º–µ—Ä—ã regex –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
PATTERNS = {
    "hypothesis": [
        r"we hypothesize(?:d)? that (.+?)[\.\;]",
        r"our hypothesis (?:is|was) (.+?)[\.\;]",
        r"we propose(?:d)? that (.+?)[\.\;]"
    ],
    "methods": [
        r"we (?:used|employed|utilized) (.+?) to",
        r"performed using (.+?)[\.\;]",
        r"measured (?:by|with|using) (.+?)[\.\;]"
    ],
    "results": [
        r"(?:showed|demonstrated|found) that (.+?)[\.\;]",
        r"p\s*[<=]\s*0\.0\d+",  # p-values
        r"\d+\.?\d*\s*¬±\s*\d+\.?\d*"  # measurements
    ]
}
```

**–î–µ—Ç–µ–∫—Ü–∏—è —Å–µ–∫—Ü–∏–π:**
```python
SECTION_HEADERS = {
    "abstract": r"^abstract$",
    "introduction": r"^introduction$",
    "methods": r"^(?:methods|materials and methods|methodology)$",
    "results": r"^results$",
    "discussion": r"^discussion$",
    "conclusion": r"^(?:conclusion|conclusions)$"
}
```

---

### 4. Hybrid Pipeline (Production Target)

**–§–∞–π–ª:** `src/pipelines/hybrid_pipeline.py`

**–¶–µ–ª—å:** –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞

**–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:**
- –°—Ç–æ–∏–º–æ—Å—Ç—å: ~$0.01-0.02 –Ω–∞ —Å—Ç–∞—Ç—å—é
- Precision: ~85% (—Ü–µ–ª–µ–≤–æ–µ)
- Recall: ~80% (—Ü–µ–ª–µ–≤–æ–µ)
- F1-score: ~82% (—Ü–µ–ª–µ–≤–æ–µ)

**–°—Ç—Ä–∞—Ç–µ–≥–∏—è:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Input Paper     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Regex   ‚îÇ ‚îÄ‚îÄ‚ñ∫ Simple patterns (Methods, Results) [FREE]
    ‚îÇ Extractor‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   NLP    ‚îÇ ‚îÄ‚îÄ‚ñ∫ Entities, dependencies (Facts) [CHEAP]
    ‚îÇ Extractor‚îÇ     spaCy: ~$0.001/paper
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   LLM    ‚îÇ ‚îÄ‚îÄ‚ñ∫ Complex reasoning (Hypotheses, Conclusions) [SELECTIVE]
    ‚îÇ(selective)‚îÇ     gpt-5-mini: Only 20% of content
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ~$0.01/paper
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Output  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**–ê–ª–≥–æ—Ä–∏—Ç–º –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π:**
```python
def extract_entity(self, text: str, entity_type: str):
    # –®–∞–≥ 1: –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å regex
    regex_result, confidence = self.regex_extractor.extract(text, entity_type)

    if confidence > 0.8:
        return regex_result  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å ‚Üí –∏—Å–ø–æ–ª—å–∑—É–µ–º regex

    # –®–∞–≥ 2: –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å NLP
    if entity_type in ["facts", "techniques"]:
        nlp_result = self.nlp_extractor.extract(text, entity_type)
        return nlp_result

    # –®–∞–≥ 3: LLM —Ç–æ–ª—å–∫–æ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
    if entity_type in ["hypotheses", "conclusions", "analysis"]:
        llm_result = self.llm_extractor.extract(text, entity_type)
        return llm_result
```

**–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è LLM –≤—ã–∑–æ–≤–æ–≤:**
- Batch processing (5-10 –∑–∞–ø—Ä–æ—Å–æ–≤ ‚Üí 1 API call)
- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ gpt-5-mini –≤–º–µ—Å—Ç–æ GPT-4

---

### 5. Data Models

**–§–∞–π–ª:** `src/models/entities.py`

```python
from dataclasses import dataclass
from typing import List, Optional
from enum import Enum

class EntityType(Enum):
    """–¢–∏–ø—ã –∏–∑–≤–ª–µ–∫–∞–µ–º—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π"""
    FACT = "fact"
    HYPOTHESIS = "hypothesis"
    EXPERIMENT = "experiment"
    TECHNIQUE = "technique"
    RESULT = "result"
    DATASET = "dataset"
    ANALYSIS = "analysis"
    CONCLUSION = "conclusion"

@dataclass
class Entity:
    """–ë–∞–∑–æ–≤–∞—è —Å—É—â–Ω–æ—Å—Ç—å"""
    id: str
    type: EntityType
    text: str
    confidence: float  # 0.0 - 1.0
    source_section: str  # Abstract, Methods, Results, etc.
    metadata: dict

@dataclass
class Relationship:
    """–°–≤—è–∑—å –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏"""
    source_id: str
    target_id: str
    relationship_type: str  # "tested_by", "based_on", "uses", etc.
    confidence: float

@dataclass
class KnowledgeGraph:
    """Knowledge graph –¥–ª—è —Å—Ç–∞—Ç—å–∏"""
    paper_id: str
    entities: List[Entity]
    relationships: List[Relationship]

    def to_networkx(self):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ NetworkX –≥—Ä–∞—Ñ"""
        import networkx as nx
        G = nx.DiGraph()

        for entity in self.entities:
            G.add_node(entity.id, **entity.__dict__)

        for rel in self.relationships:
            G.add_edge(rel.source_id, rel.target_id,
                      type=rel.relationship_type,
                      confidence=rel.confidence)

        return G
```

---

### 6. Validation & Metrics

**–§–∞–π–ª:** `src/validation/metrics.py`

```python
from dataclasses import dataclass
from typing import List, Dict
from src.models.entities import Entity

@dataclass
class ValidationMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
    precision: float
    recall: float
    f1_score: float

    # –ü–æ —Ç–∏–ø–∞–º —Å—É—â–Ω–æ—Å—Ç–µ–π
    per_entity_metrics: Dict[str, Dict[str, float]]

    # Aggregate
    total_tp: int  # True Positives
    total_fp: int  # False Positives
    total_fn: int  # False Negatives

class MetricsCalculator:
    """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞"""

    def compute_metrics(
        self,
        predicted: List[Entity],
        ground_truth: List[Entity]
    ) -> ValidationMetrics:
        """
        –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å ground truth

        Args:
            predicted: –°—É—â–Ω–æ—Å—Ç–∏, –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –ø–∞–π–ø–ª–∞–π–Ω–æ–º
            ground_truth: –≠—Ç–∞–ª–æ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏

        Returns:
            ValidationMetrics
        """
        # Matching logic (—Å —É—á–µ—Ç–æ–º —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞)
        tp, fp, fn = self._calculate_matches(predicted, ground_truth)

        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

        # Per-entity breakdown
        per_entity = self._calculate_per_entity_metrics(predicted, ground_truth)

        return ValidationMetrics(
            precision=precision,
            recall=recall,
            f1_score=f1,
            per_entity_metrics=per_entity,
            total_tp=tp,
            total_fp=fp,
            total_fn=fn
        )

    def _calculate_matches(self, predicted, ground_truth):
        """Fuzzy matching —Å –ø–æ—Ä–æ–≥–æ–º similarity"""
        from difflib import SequenceMatcher

        tp = fp = fn = 0
        matched_gt = set()

        for pred in predicted:
            best_match = None
            best_score = 0

            for idx, gt in enumerate(ground_truth):
                if idx in matched_gt:
                    continue

                if pred.type != gt.type:
                    continue

                # Text similarity
                similarity = SequenceMatcher(None, pred.text, gt.text).ratio()

                if similarity > best_score:
                    best_score = similarity
                    best_match = idx

            if best_score > 0.8:  # Threshold –¥–ª—è —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                tp += 1
                matched_gt.add(best_match)
            else:
                fp += 1

        fn = len(ground_truth) - len(matched_gt)

        return tp, fp, fn
```

---

### 7. Storage Layer

**–§–∞–π–ª:** `src/storage/results_db.py`

```python
import sqlite3
import json
from typing import List, Optional
from src.models.results import ExtractionResult

class ResultsDatabase:
    """SQLite database –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""

    def __init__(self, db_path: str = "data/results.db"):
        self.db_path = db_path
        self._init_db()

    def _init_db(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("""
            CREATE TABLE IF NOT EXISTS extraction_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                paper_id TEXT NOT NULL,
                pipeline_name TEXT NOT NULL,
                entities TEXT,  -- JSON
                relationships TEXT,  -- JSON
                metadata TEXT,  -- JSON
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(paper_id, pipeline_name)
            )
        """)

        cursor.execute("""
            CREATE TABLE IF NOT EXISTS ground_truth (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                paper_id TEXT UNIQUE NOT NULL,
                entities TEXT,  -- JSON
                relationships TEXT,  -- JSON
                annotator TEXT,
                annotation_date DATETIME
            )
        """)

        conn.commit()
        conn.close()

    def save_result(self, result: ExtractionResult, pipeline_name: str):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("""
            INSERT OR REPLACE INTO extraction_results
            (paper_id, pipeline_name, entities, relationships, metadata)
            VALUES (?, ?, ?, ?, ?)
        """, (
            result.paper_id,
            pipeline_name,
            json.dumps(result.entities),
            json.dumps(result.relationships),
            json.dumps(result.metadata)
        ))

        conn.commit()
        conn.close()

    def get_result(self, paper_id: str, pipeline_name: str) -> Optional[ExtractionResult]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —Å—Ç–∞—Ç—å–∏ –∏ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("""
            SELECT entities, relationships, metadata
            FROM extraction_results
            WHERE paper_id = ? AND pipeline_name = ?
        """, (paper_id, pipeline_name))

        row = cursor.fetchone()
        conn.close()

        if row:
            return ExtractionResult(
                paper_id=paper_id,
                entities=json.loads(row[0]),
                relationships=json.loads(row[1]),
                metadata=json.loads(row[2])
            )
        return None
```

---

### 8. Streamlit UI

**–§–∞–π–ª:** `ui/app.py`

```python
import streamlit as st
from src.pipelines.orchestrator import PipelineOrchestrator
from src.parsers import get_parser
from ui.components.graph_viewer import display_knowledge_graph
from ui.components.metrics_dashboard import display_metrics

st.set_page_config(
    page_title="Singularis Knowledge Extractor",
    page_icon="üß¨",
    layout="wide"
)

# Sidebar
st.sidebar.title("‚öôÔ∏è Configuration")
pipeline_choice = st.sidebar.selectbox(
    "Select Pipeline",
    ["LLM (Ground Truth)", "Regex-based", "Hybrid (Recommended)"],
    index=2
)

# Cost indicator
costs = {
    "LLM (Ground Truth)": "$0.30",
    "Regex-based": "$0.00",
    "Hybrid (Recommended)": "$0.02"
}
st.sidebar.metric("Estimated Cost/Paper", costs[pipeline_choice])

# Main area
st.title("üß¨ Singularis Knowledge Extractor")
st.markdown("Extract structured knowledge from scientific papers")

# File upload
uploaded_file = st.file_uploader(
    "Upload Scientific Paper",
    type=["pdf", "txt", "html"],
    help="Supported formats: PDF, TXT, HTML"
)

if uploaded_file:
    # Parse document
    parser = get_parser(uploaded_file.type)
    paper_text = parser.parse(uploaded_file)

    # Display paper info
    st.info(f"üìÑ Paper loaded: {len(paper_text)} characters")

    # Extract button
    if st.button("üöÄ Extract Knowledge", type="primary"):
        with st.spinner("Processing paper..."):
            orchestrator = PipelineOrchestrator()
            result = orchestrator.run_pipeline(
                pipeline_choice,
                paper_text,
                uploaded_file.name
            )

        # Display results in tabs
        tab1, tab2, tab3, tab4 = st.tabs([
            "üìä Entities",
            "üîó Knowledge Graph",
            "üìà Metrics",
            "üî¨ Raw JSON"
        ])

        with tab1:
            st.subheader("Extracted Entities")
            for entity_type, entities in result.entities.items():
                with st.expander(f"{entity_type.title()} ({len(entities)})"):
                    for entity in entities:
                        st.markdown(f"- {entity['text']}")

        with tab2:
            st.subheader("Knowledge Graph")
            display_knowledge_graph(result.relationships, result.entities)

        with tab3:
            st.subheader("Pipeline Metrics")
            display_metrics(result.metadata)

        with tab4:
            st.json(result.entities)
```

---

## üìÖ Implementation Timeline

### Week 1: Infrastructure & Ground Truth (7-13 –æ–∫—Ç—è–±—Ä—è)

**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** –°–æ–∑–¥–∞–Ω–∏–µ foundation + LLM pipeline

**–ó–∞–¥–∞—á–∏:**
1. ‚úÖ **–ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö** (`src/models/`)
   - `entities.py` - Entity, Relationship, KnowledgeGraph
   - `results.py` - ExtractionResult, PipelineMetrics

2. ‚úÖ **Base Pipeline** (`src/pipelines/base_pipeline.py`)
   - –ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –∫–ª–∞—Å—Å BasePipeline
   - –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã –¥–ª—è –≤—Å–µ—Ö –ø–∞–π–ø–ª–∞–π–Ω–æ–≤

3. ‚úÖ **LLM Pipeline** (`src/pipelines/llm_pipeline.py`)
   - –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ LLM –∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏
   - –ü—Ä–æ–º–ø—Ç-–∏–Ω–∂–∏–Ω–∏—Ä–∏–Ω–≥ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
   - JSON-—Ä–µ–∂–∏–º –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞

4. ‚úÖ **–ü–∞—Ä—Å–µ—Ä—ã** (`src/parsers/`)
   - `pdf_parser.py` (PyMuPDF + pdfplumber)
   - `txt_parser.py`
   - `factory.py`

5. ‚úÖ **Storage** (`src/storage/`)
   - `results_db.py` - SQLite –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
   - `ground_truth.py` - Ground truth database

6. ‚úÖ **–°–æ–∑–¥–∞–Ω–∏–µ Ground Truth**
   - –û–±—Ä–∞–±–æ—Ç–∞—Ç—å 10-15 —Å—Ç–∞—Ç–µ–π —á–µ—Ä–µ–∑ LLM Pipeline
   - –†—É—á–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (—Å —ç–∫—Å–ø–µ—Ä—Ç–æ–º)
   - –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ `ground_truth/`

**Deliverable:** Ground truth –¥–∞—Ç–∞—Å–µ—Ç (10-15 —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π)

---

### Week 2: Cost-Efficient Pipelines (14-20 –æ–∫—Ç—è–±—Ä—è)

**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** Regex + Hybrid pipelines + –≤–∞–ª–∏–¥–∞—Ü–∏—è

**–ó–∞–¥–∞—á–∏:**
1. ‚úÖ **Extractors** (`src/extractors/`)
   - `regex_extractor.py` - –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è 8 —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π
   - `nlp_extractor.py` - spaCy/scispaCy –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
   - `section_detector.py` - –î–µ—Ç–µ–∫—Ü–∏—è —Å–µ–∫—Ü–∏–π —Å—Ç–∞—Ç—å–∏

2. ‚úÖ **Regex Pipeline** (`src/pipelines/regex_pipeline.py`)
   - –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —á–∏—Å—Ç–æ –ø–∞—Ç—Ç–µ—Ä–Ω-based –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
   - –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ ground truth

3. ‚úÖ **Hybrid Pipeline** (`src/pipelines/hybrid_pipeline.py`)
   - –ö–æ–º–±–∏–Ω–∞—Ü–∏—è Regex + NLP + selective LLM
   - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å—Ç–æ–∏–º–æ—Å—Ç–∏ (batch processing, caching)

4. ‚úÖ **Validation** (`src/validation/`)
   - `metrics.py` - –†–∞—Å—á–µ—Ç Precision, Recall, F1
   - `comparator.py` - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–æ–≤

5. ‚úÖ **Pipeline Orchestrator** (`src/pipelines/orchestrator.py`)
   - –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞–º–∏
   - –°–±–æ—Ä –º–µ—Ç—Ä–∏–∫

6. ‚úÖ **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ ground truth**
   - –ü—Ä–æ–≥–Ω–∞—Ç—å –≤—Å–µ 3 –ø–∞–π–ø–ª–∞–π–Ω–∞ –Ω–∞ 10-15 —Å—Ç–∞—Ç—å—è—Ö
   - –°–æ–±—Ä–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏

**Deliverable:** –†–∞–±–æ—á–∏–µ Regex –∏ Hybrid pipelines —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏

---

### Week 3: UI, Optimization & Deployment (21-22 –æ–∫—Ç—è–±—Ä—è)

**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è + —Ñ–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è

**–ó–∞–¥–∞—á–∏:**
1. ‚úÖ **Streamlit UI** (`ui/`)
   - `app.py` - Main application
   - `components/graph_viewer.py` - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞
   - `components/metrics_dashboard.py` - –î–∞—à–±–æ—Ä–¥ –º–µ—Ç—Ä–∏–∫

2. ‚úÖ **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è**
   - –¢—é–Ω–∏–Ω–≥ regex –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—à–∏–±–æ–∫
   - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è LLM —á–∞—Å—Ç–∏
   - Batch processing –¥–ª—è API –≤—ã–∑–æ–≤–æ–≤

3. ‚úÖ **–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è**
   - README —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏
   - API documentation
   - Performance metrics report

4. ‚úÖ **Deployment**
   - –î–µ–ø–ª–æ–π –Ω–∞ Streamlit Cloud / Hugging Face Spaces
   - –ü—É–±–ª–∏—á–Ω—ã–π URL –¥–ª—è –∂—é—Ä–∏

5. ‚úÖ **–í–∏–¥–µ–æ-–¥–µ–º–æ** (3-5 –º–∏–Ω—É—Ç)
   - –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç—å–∏
   - –í—ã–±–æ—Ä –ø–∞–π–ø–ª–∞–π–Ω–∞
   - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
   - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫

**Deliverable:** Production-ready –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ + –≤–∏–¥–µ–æ-–¥–µ–º–æ

---

## üéØ Success Metrics

### –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏
- ‚úÖ **Precision:** ‚â• 85%
- ‚úÖ **Recall:** ‚â• 80%
- ‚úÖ **F1-score:** ‚â• 82%
- ‚úÖ **Cost:** < $0.05 –Ω–∞ —Å—Ç–∞—Ç—å—é (—Ü–µ–ª–µ–≤–æ–µ: $0.02)
- ‚úÖ **Speed:** > 100 —Å—Ç–∞—Ç–µ–π/—á–∞—Å

### –ö—Ä–∏—Ç–µ—Ä–∏–∏ –∂—é—Ä–∏
- ‚úÖ **–ü–æ–ª–Ω–æ—Ç–∞ –∏ –¢–æ—á–Ω–æ—Å—Ç—å (25%):** –ú–µ—Ç—Ä–∏–∫–∏ –≤—ã—à–µ
- ‚úÖ **–†–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å (25%):** –†–∞–±–æ—Ç–∞ —Å PDF/TXT/HTML
- ‚úÖ **–°—Ç–æ–∏–º–æ—Å—Ç—å (25%):** $0.02/—Å—Ç–∞—Ç—å—è vs $0.30 baseline
- ‚úÖ **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (25%):** 100+ —Å—Ç–∞—Ç–µ–π/—á–∞—Å

### Bonus Points
- ‚úÖ **–ê–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥:** Hybrid pipeline –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–Ω–∏–∂–µ–Ω–∏–µ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –≤ 15x –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞
- ‚úÖ **–ò–Ω–Ω–æ–≤–∞—Ü–∏–∏:** –ú–æ–¥—É–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–æ–≤

---

## üîß Technical Stack (Updated)

### Core (Existing)
- Python 3.10+
- OpenAI API (GPT-4, gpt-5-mini)
- Ollama (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤)

### New Dependencies
```txt
# Parsing
PyMuPDF>=1.23.0        # PDF extraction
pdfplumber>=0.10.0     # Tables from PDF
beautifulsoup4>=4.12.0 # HTML parsing

# NLP
spacy>=3.7.0
scispacy>=0.5.0        # Scientific NLP
en-core-sci-sm         # SciBERT model

# Vector DB
chromadb>=0.4.0

# Storage
sqlite3 (built-in)

# UI
streamlit>=1.28.0
plotly>=5.17.0         # Interactive graphs
networkx>=3.2.0        # Graph processing

# Utils
python-dotenv>=1.0.0
pyyaml>=6.0.1
```

---

## üí° Key Optimizations

### 1. LLM Cost Reduction
```python
# –ü–ª–æ—Ö–æ: –ö–∞–∂–¥—É—é —Å—É—â–Ω–æ—Å—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ
for section in sections:
    llm.generate(f"Extract hypothesis from: {section}")

# –•–æ—Ä–æ—à–æ: Batch processing
combined_prompt = "\n---\n".join([
    f"Section {i}: {section}"
    for i, section in enumerate(sections)
])
result = llm.generate(f"Extract hypotheses from all sections:\n{combined_prompt}")
```

### 2. Regex Patterns Optimization
```python
# –ò—Å–ø–æ–ª—å–∑—É–µ–º compiled regex –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
import re

class RegexExtractor:
    def __init__(self):
        # Compile patterns once
        self.hypothesis_patterns = [
            re.compile(pattern, re.IGNORECASE)
            for pattern in HYPOTHESIS_PATTERNS
        ]

    def extract(self, text):
        # Reuse compiled patterns
        for pattern in self.hypothesis_patterns:
            matches = pattern.findall(text)
            # ...
```

### 3. Caching
```python
from functools import lru_cache

class HybridPipeline:
    @lru_cache(maxsize=100)
    def _extract_with_llm(self, text_hash: str, entity_type: str):
        """Cache LLM results for identical texts"""
        # LLM call here
```

---

## üìä Expected Results

### Pipeline Comparison (Projected)

| Pipeline | Cost/Paper | Precision | Recall | F1 | Speed |
|----------|-----------|-----------|--------|-------|-------|
| LLM (GPT-4) | $0.30 | 95% | 92% | 93.5% | 30/hour |
| Regex | $0.00 | 65% | 55% | 59.6% | 300/hour |
| **Hybrid** | **$0.02** | **85%** | **80%** | **82.4%** | **120/hour** |

### Bonus Points Justification
- **15x cost reduction** (from $0.30 to $0.02)
- **Quality retention** (93.5% F1 ‚Üí 82.4% F1, only 11% drop)
- **Algorithmic innovation** (–º–æ–¥—É–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞)

---

## üöÄ Next Steps

1. ‚úÖ **Approve this plan** ‚Üê You are here
2. üìù **Week 1:** Implement infrastructure + LLM pipeline
3. üìù **Week 2:** Implement Regex + Hybrid pipelines
4. üìù **Week 3:** UI + deployment + video
5. üéâ **Submit before October 22, 11:59 PM PT**

---

**Last Updated:** October 10, 2025
**Status:** ‚úÖ Ready for Implementation
**Estimated Total Dev Time:** 60-80 hours over 3 weeks
