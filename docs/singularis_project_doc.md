# 🚀 Singularis Challenge - Project Documentation

## 📋 Информация о хакатоне

**Хакатон:** Agentic AI Against Aging  
**Сайт:** https://www.hackaging.ai/  
**Discord:** Основная платформа коммуникации  
**Даты:** 7-24 октября 2025  
**Дедлайн:** 22 октября, 11:59 PM PT (Code Freeze)  
**Призовой фонд:** $20,000  
**Формат:** Онлайн + финал в Сан-Франциско

### Timeline хакатона
| Дата | Событие |
|------|---------|
| **7 октября** | Kick-off Event (9 AM PT / 6 PM CET) |
| **7-9 октября** | Challenge Deep-Dive Q&A |
| **8-22 октября** | Hacking Period (14 дней) |
| **22 октября** | Code Freeze (11:59 PM PT) |
| **После 22 октября** | Finals - оценка жюри |

---

## 🎯 Singularis Challenge - Обзор

**Владелец челленджа:** Peter Lidsky  
**Трек:** Fundamental Track

### Миссия
Революционизировать формат научных публикаций через создание knowledge graphs из миллионов научных статей по старению.

### Контекст проблемы
**Исследования старения сегодня:**
- Фрагментированы и недофинансированы
- Клинические испытания занимают десятилетия
- Персонализированная медицина в зачаточном состоянии
- Большинство людей не имеют доступа к передовым методам

**Проблемы научных публикаций:**
- Трудно анализировать в большом объеме
- Информация не структурирована
- Связи между исследованиями неочевидны
- Невозможно быстро извлечь ключевые данные

### Решение
- AI-агенты для обработки огромных объемов исследовательских данных
- Система автоматического извлечения структурированной информации
- Построение графов знаний с использованием экономичных методов
- Open-source решения, доступные каждому

---

## 🎯 Цели проекта

### Основная задача
Извлечь из научных статей следующие элементы:
- ✅ **Факты** (установленные научные данные)
- ✅ **Гипотезы** (предположения исследователей)
- ✅ **Эксперименты** (методология и дизайн)
- ✅ **Методы** (использованные техники)
- ✅ **Результаты** (полученные данные)
- ✅ **Датасеты** (использованные данные)
- ✅ **Анализы** (статистические методы)
- ✅ **Заключения** (выводы исследования)

### Построение Knowledge Graph
Создать связи между элементами:
- Факт → поддерживается → Экспериментом
- Гипотеза → тестируется → Методом
- Результат → анализируется → Анализом
- Заключение → основано на → Результатах

---

## 🔑 Ключевые требования

### 1. ⚡ Экономичность (КРИТИЧНО!)
- Минимальная стоимость обработки одной статьи
- Система должна обрабатывать **миллионы статей**
- Оптимизация использования дорогих API
- Предсказуемая стоимость масштабирования

### 2. 🔧 Гибридный подход
❌ **НЕ использовать:** Чисто LLM-подход (слишком дорого)  
✅ **Использовать:** 
- LLM для сложных задач (классификация, извлечение связей)
- Регулярные выражения для паттернов
- Алгоритмы NLP для предобработки
- Heuristics для простых случаев

### 3. 📈 Масштабируемость
- Обработка тысяч статей параллельно
- Очереди задач для асинхронной обработки
- Кэширование результатов
- Инкрементальное обновление графа

### 4. 🎯 Робастность
- Работа с разными форматами (PDF, HTML, XML)
- Обработка ошибок OCR
- Устойчивость к различным стилям написания
- Поддержка разных журналов и форматов публикаций

---

## 📊 Критерии оценки

### 1. Полнота и точность (25%)
- **Precision:** Доля правильно извлеченных элементов
- **Recall:** Доля найденных от всех существующих элементов
- **F1-score:** Гармоническое среднее precision и recall

### 2. Стоимость обработки (25%)
- **$/статья:** Общие затраты на обработку
- **CPU/GPU часы:** Вычислительные ресурсы
- **API costs:** Затраты на сторонние сервисы

### 3. Пропускная способность (25%)
- **Статей/час:** Скорость обработки
- **Параллелизм:** Количество одновременных задач
- **Latency:** Время обработки одной статьи

### 4. Робастность (25%)
- **Форматы:** PDF, HTML, XML, LaTeX
- **Error handling:** Обработка некорректных данных
- **Consistency:** Стабильность результатов

---

## 🛠️ Технический стек (утверждено ментором)

### Core Stack
- **Python 3.10+** - основной язык
- **ChromaDB** - векторная база данных (простой setup)
- **gpt-5-mini API** - основная LLM (экономично)
- **LangChain** - фреймворк для RAG системы
- **OpenAI Embeddings** - для векторизации текста

### Парсинг документов
- **PyMuPDF (fitz)** - PDF extraction
- **BeautifulSoup4** - HTML parsing
- **pdfplumber** - таблицы из PDF
- **Grobid** (опционально) - структурирование научных статей

### Backend (для расширения)
- **FastAPI** - веб-фреймворк
- **Uvicorn** - ASGI сервер

### Frontend (опционально)
- **Streamlit** - быстрый UI для прототипа
- **Gradio** - альтернатива для демо

### DevOps
- **Docker** - контейнеризация
- **Docker Compose** - оркестрация сервисов

---

## 🏗️ Архитектура решения (RAG система)

### Подход: RAG (Retrieval-Augmented Generation)

RAG система для извлечения структурированной информации из научных статей.

### Pipeline обработки (v1.0 - MVP)

```
1. Document Ingestion
   ├─ PDF Upload / URL
   ├─ Text extraction (PyMuPDF)
   ├─ Chunking (LangChain TextSplitter)
   └─ Section detection (Abstract, Methods, Results, etc.)

2. Embedding & Storage
   ├─ OpenAI Embeddings (text-embedding-3-small)
   ├─ Store in ChromaDB
   ├─ Metadata: (section, authors, journal, etc.)
   └─ Document ID mapping

3. Retrieval Layer
   ├─ Query vectorization
   ├─ Semantic search (ChromaDB)
   ├─ Context retrieval (top-k chunks)
   └─ Re-ranking (опционально)

4. Extraction Layer (gpt-5-mini)
   ├─ Prompt engineering для extraction
   ├─ Structured output (JSON)
   ├─ Extract: Facts, Hypotheses, Methods, Results, etc.
   └─ Validation & post-processing

5. Knowledge Graph (v2.0)
   ├─ Entity extraction
   ├─ Relationship mapping
   ├─ Graph construction
   └─ Visualization

6. Output Layer
   ├─ Structured JSON
   ├─ REST API (FastAPI)
   └─ Web interface (Streamlit)
```

### Архитектура компонентов

```
┌─────────────────────────────────────────────────────┐
│                   User Interface                     │
│              (Streamlit / Gradio)                    │
└────────────────────┬────────────────────────────────┘
                     │
┌────────────────────▼────────────────────────────────┐
│                  FastAPI Backend                     │
│  ┌──────────────────────────────────────────────┐  │
│  │         Document Processing Service           │  │
│  │  (PyMuPDF → Chunking → Embedding)            │  │
│  └──────────────────┬───────────────────────────┘  │
│                     │                                │
│  ┌──────────────────▼───────────────────────────┐  │
│  │           Vector Store (ChromaDB)             │  │
│  │  - Embeddings (OpenAI)                        │  │
│  │  - Metadata                                    │  │
│  │  - Similarity search                           │  │
│  └──────────────────┬───────────────────────────┘  │
│                     │                                │
│  ┌──────────────────▼───────────────────────────┐  │
│  │         RAG Query Engine (LangChain)          │  │
│  │  - Retrieval                                   │  │
│  │  - gpt-5-mini для extraction                 │  │
│  │  - Structured output                           │  │
│  └──────────────────┬───────────────────────────┘  │
│                     │                                │
│  ┌──────────────────▼───────────────────────────┐  │
│  │           Extraction Results                   │  │
│  │  (Facts, Hypotheses, Methods, etc.)           │  │
│  └────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────┘
```

---

## 💡 Стратегия разработки (утверждено командой)

### Фазы развития

#### Phase 1: MVP (Week 1) - RAG на эмбеддингах
```python
# Простая RAG система с LangChain
1. Document loading (PDF → Text)
2. Text chunking (RecursiveCharacterTextSplitter)
3. Embeddings (OpenAI text-embedding-3-small)
4. Store in ChromaDB
5. Query → Retrieve → gpt-5-mini extraction
```

**Цель:** Работающий прототип с базовым extraction

#### Phase 2: Улучшение качества (Week 2)
- Оптимизация промптов для extraction
- Добавление metadata фильтров
- Улучшение chunking стратегии
- Post-processing и валидация результатов
- **Оценка качества: Татьяна (эксперт)**

#### Phase 3: Расширение (Week 3)
- Knowledge Graph (если останется время)
- Batch processing для множества статей
- Веб-интерфейс
- Метрики и аналитика

### Стратегия экономии

#### 1. Использование gpt-5-mini
- Самая экономичная модель OpenAI
- Достаточная для extraction задач
- ~$0.15 за 1M input tokens
- ~$0.60 за 1M output tokens

#### 2. Эффективный chunking
```python
# Оптимальные параметры
chunk_size = 1000  # токены
chunk_overlap = 200  # перекрытие для контекста
```

#### 3. Caching embeddings
- Кэширование embeddings в ChromaDB
- Повторное использование для похожих документов
- Избегать re-embedding одних и тех же секций

#### 4. Batch processing
- Группировать запросы к API
- Использовать async для параллельных запросов
- Rate limiting для контроля затрат

---

## 📝 Требования к подаче (Deadline: 22 октября)

### Обязательные материалы

1. **🎥 Видео-демо (3-5 минут)**
   - Демонстрация работы системы
   - Примеры извлечения данных
   - Визуализация knowledge graph
   - Акцент на критериях оценки

2. **💻 Репозиторий кода**
   - README с инструкциями
   - Документация API
   - Примеры использования
   - Открытый исходный код (рекомендуется)

3. **📄 Описание проекта**
   - Технический подход
   - Архитектура решения
   - Анализ затрат
   - Метрики качества

4. **🌐 Развернутое решение**
   - Публичный URL
   - Работающий демо
   - Жюри НЕ будет запускать код локально

---

## 📈 Метрики успеха

### Целевые показатели
- **Precision:** ≥ 85%
- **Recall:** ≥ 80%
- **F1-score:** ≥ 82%
- **Стоимость:** < $0.05 на статью
- **Скорость:** > 100 статей/час
- **Uptime:** > 99%

---

## 🚀 Plan разработки (обновлённый)

### Week 1 (7-13 октября) - MVP RAG системы
- [x] Определить архитектуру - **RAG система**
- [x] Выбрать стек - **ChromaDB + gpt-5-mini + LangChain**
- [ ] Setup окружения
  - [ ] Установить Python 3.10+
  - [ ] Setup ChromaDB
  - [ ] Получить OpenAI API key
  - [ ] Установить LangChain
- [ ] Реализовать базовый pipeline
  - [ ] PDF → Text extraction
  - [ ] Text chunking
  - [ ] Embeddings + ChromaDB storage
  - [ ] Simple query interface
- [ ] Протестировать на 3-5 статьях
- [ ] **Milestone:** Работающий MVP с базовым extraction

### Week 2 (14-20 октября) - Улучшение качества
- [ ] Оптимизация промптов для extraction
  - [ ] Промпт для Facts
  - [ ] Промпт для Hypotheses
  - [ ] Промпт для Methods
  - [ ] Промпт для Results
- [ ] Улучшение retrieval
  - [ ] Metadata фильтры
  - [ ] Section-aware search
  - [ ] Re-ranking (опционально)
- [ ] Тестирование на 20-30 статьях
- [ ] **Оценка качества: Татьяна**
- [ ] Итерация на основе фидбека
- [ ] **Milestone:** Система с хорошей точностью

### Week 3 (21-22 октября) - Финализация
- [ ] Создать веб-интерфейс (Streamlit)
- [ ] Batch processing для демо
- [ ] Собрать метрики
  - [ ] Precision, Recall, F1
  - [ ] Стоимость обработки
  - [ ] Время обработки
- [ ] Деплой на Hugging Face Spaces / Streamlit Cloud
- [ ] Записать видео-демо (3-5 минут)
- [ ] Подготовить документацию
- [ ] **Submit до 22 октября, 11:59 PM PT**

### Роли в команде
- **Разработка:** Основной pipeline и код
- **Оценка качества:** Татьяна (эксперт по scientific papers)
- **Менторинг:** Доступен через Discord

---

## 🔗 Полезные ресурсы

### Датасеты и API
- **PubMed Central:** Бесплатный доступ к full-text статьям
- **bioRxiv / medRxiv:** Preprints
- **Semantic Scholar API:** Метаданные статей
- **Europe PMC:** Европейская база медицинских статей

### Tools & Libraries
- **Grobid:** https://github.com/kermitt2/grobid - структурирование научных статей
- **SciSpacy:** https://github.com/allenai/scispacy - NLP для научных текстов
- **Neo4j:** https://neo4j.com/ - графовая база данных
- **LangChain:** https://python.langchain.com/ - оркестрация LLM

### Примеры и референсы
- **OpenAlex:** https://openalex.org/ - knowledge graph научных работ
- **CORD-19 Dataset:** Пример обработки научных статей о COVID-19
- **Semantic Scholar Graph:** Inspiration для архитектуры

### Записи сессий
- Kick-off Event (7 октября) - будет доступна в Discord
- Challenge Deep-Dive Q&A (7-9 октября) - записи в канале челленджа

---

## 📞 Поддержка

### Контакты
- **Discord:** Основная платформа коммуникации
- **Channel:** #singularis-challenge
- **Вопросы:** DM @HackAging.ai
- **Менторы:** @Organizer или @Mentor (доступны для помощи)

### Правила участия
- Соблюдать вежливость и SFW контент
- Уважать приватность, не делиться персональными данными
- Не давать медицинских советов
- Следовать ToS Discord и Code of Conduct

### Рекомендации по коммуникации
- Использовать английский язык в общих каналах
- Создавать threads для подтем
- Добавлять контекст (что/зачем/результат)
- Фиксировать решения и следующие шаги
- Учитывать разные часовые пояса

---

## 🚀 Быстрый старт

### Шаг 1: Присоединиться
1. Представиться в канале **#introduce-yourself**
2. Изучить челлендж в категории **CHALLENGES**
3. Найти команду через @HackAging.ai (опционально)
4. Присоединиться к обсуждениям в **#singularis-challenge**
5. Следить за обновлениями в **#announcements**

### Шаг 2: Начать разработку
1. Изучить требования и критерии оценки
2. Выбрать технический стек
3. Создать репозиторий и настроить окружение
4. Начать с MVP парсера

### Шаг 3: Подготовить подачу
1. Разработать working solution
2. Задеплоить на публичный URL
3. Записать видео-демо (3-5 минут)
4. Подготовить документацию
5. Submit до **22 октября, 11:59 PM PT**

---

**Good luck! 🚀**

*Документ обновлен: 7 октября 2025*