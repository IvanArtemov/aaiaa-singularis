# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

---

## üìã –û –ø—Ä–æ–µ–∫—Ç–µ

**–ü—Ä–æ–µ–∫—Ç:** Singularis Challenge - –†–µ—Ñ–æ—Ä–º–∞ –Ω–∞—É—á–Ω–æ–≥–æ –ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∏—è
**–•–∞–∫–∞—Ç–æ–Ω:** Agentic AI Against Aging (https://www.hackaging.ai/)
**–î–µ–¥–ª–∞–π–Ω:** 22 –æ–∫—Ç—è–±—Ä—è 2025, 11:59 PM PT (Code Freeze)
**–ü—Ä–∏–∑–æ–≤–æ–π —Ñ–æ–Ω–¥:** $20,000

### –ú–∏—Å—Å–∏—è Singularis
–ò–∑–º–µ–Ω–∏—Ç—å —Å–ø–æ—Å–æ–± –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —É—á–µ–Ω—ã—Ö —Å–æ –∑–Ω–∞–Ω–∏—è–º–∏. –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø—É–±–ª–∏–∫—É–µ–º–∞—è –µ–¥–∏–Ω–∏—Ü–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å **–º–µ–Ω—å—à–µ –Ω–∞—É—á–Ω–æ–π —Å—Ç–∞—Ç—å–∏** ‚Äî —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–¥–Ω–∞ –≥–∏–ø–æ—Ç–µ–∑–∞, —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç, –º–µ—Ç–æ–¥, —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç.

### –ó–∞–¥–∞—á–∞
–°–æ–∑–¥–∞—Ç—å **cost-efficient** —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ **50 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π** –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è knowledge graph, –≥–¥–µ —Å—Ç–∞—Ç—å–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –∫–∞–∫ –≥—Ä–∞—Ñ—ã –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∞–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤.

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏–∑–≤–ª–µ–∫–∞–µ–º—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
1. **Input Fact** - –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–µ –∑–Ω–∞–Ω–∏–µ, –≤—Ö–æ–¥—è—â–µ–µ –≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
2. **Hypothesis** - –ù–∞—É—á–Ω–æ–µ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
3. **Experiment** - –ü—Ä–æ—Ü–µ–¥—É—Ä–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≥–∏–ø–æ—Ç–µ–∑—ã
4. **Technique/Method** - –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –º–µ—Ç–æ–¥—ã –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
5. **Result** - –ü–æ–ª—É—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è
6. **Dataset** - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
7. **Analysis** - –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è/–≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
8. **Conclusion** - –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –∏ –≤—ã–≤–æ–¥—ã

### –ö–ª—é—á–µ–≤—ã–µ —Å–≤—è–∑–∏
- Hypothesis ‚Üí tested by ‚Üí Experiment
- Result ‚Üí analyzed using ‚Üí Analysis
- Conclusion ‚Üí based on ‚Üí Result
- Method ‚Üí applied in ‚Üí Experiment

---

## üîë –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

### ‚ö° –≠–ö–û–ù–û–ú–ò–ß–ù–û–°–¢–¨ (–ì–õ–ê–í–ù–´–ô –ü–†–ò–û–†–ò–¢–ï–¢!)
- –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏
- –¶–µ–ª–µ–≤–∞—è –º–µ—Ç—Ä–∏–∫–∞: **< $0.05 –Ω–∞ —Å—Ç–∞—Ç—å—é**
- –°–∏—Å—Ç–µ–º–∞ –¥–æ–ª–∂–Ω–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ **–º–∏–ª–ª–∏–æ–Ω—ã —Å—Ç–∞—Ç–µ–π**
- ‚ùå –ù–ï –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —á–∏—Å—Ç–æ LLM-–ø–æ–¥—Ö–æ–¥
- ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥: LLM + regex + NLP + heuristics

### üìä –¶–µ–ª–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
- **Precision:** ‚â• 85%
- **Recall:** ‚â• 80%
- **F1-score:** ‚â• 82%
- **–°—Ç–æ–∏–º–æ—Å—Ç—å:** < $0.05 –Ω–∞ —Å—Ç–∞—Ç—å—é
- **–°–∫–æ—Ä–æ—Å—Ç—å:** > 100 —Å—Ç–∞—Ç–µ–π/—á–∞—Å
- **–ú–∞—Å—à—Ç–∞–±:** –ü—Ä–æ–µ–∫—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –Ω–∞ 50M —Å—Ç–∞—Ç–µ–π

---

## üíª Development Commands

### Installation
```bash
# Install all dependencies
pip install -r requirements.txt

# Set up environment variables
cp .env.example .env
# Edit .env and add your OPENAI_API_KEY
```

### Running Tests
```bash
# Run all tests
pytest

# Run with verbose output
pytest -v

# Run only integration tests
pytest tests/integration/ -v

# Run with coverage report
pytest --cov=src --cov-report=term-missing

# Generate HTML coverage report
pytest --cov=src --cov-report=html
# Open htmlcov/index.html in browser
```

### Running Examples

```bash
# Test LLM adapters (OpenAI/Ollama)
python scripts/example_adapters.py

# Test PubMed fetcher
python scripts/example_fetchers.py

# Test arXiv fetcher
python scripts/example_arxiv.py

# Download PDFs from PubMed
python scripts/download_pdfs_demo.py

# Extract PDFs from PMC packages
python scripts/extract_pdfs_from_packages.py

# Batch download by topic (PubMed)
python scripts/batch_download_cross_referenced.py

# Batch download KG papers (arXiv)
python scripts/batch_download_arxiv_kg.py

# Test PDF parser
python scripts/example_pdf_parser.py

# Test LLM extraction pipeline
python scripts/example_llm_pipeline.py

# Generate SVG knowledge graph
python scripts/generate_svg.py results/sample_result.json
```

### Configuration

**Switch LLM Provider (OpenAI ‚Üî Ollama):**
Edit `src/config/llm_config.yaml`:
```yaml
active_provider: "openai"  # or "ollama"
```

**PubMed API Key (optional but recommended):**
Add to `.env`:
```
NCBI_API_KEY=your_api_key_here
```
This increases rate limit from 3 req/sec to 10 req/sec.

---

## üèóÔ∏è Code Architecture

### High-Level Design Principles

**1. Factory Pattern for Extensibility**
- `get_llm_adapter(provider)` - Create LLM adapters (OpenAI, Ollama, etc.)
- `get_fetcher(type)` - Create paper fetchers (PubMed, PMC, etc.)
- `get_parser(format)` - Create document parsers (PDF, TXT, HTML)

**2. Pipeline Abstraction**
All extraction pipelines inherit from `BasePipeline`:
- `LLMPipeline` - High-quality extraction using GPT (~$0.03-$0.30/paper)
- `RegexPipeline` - Pattern-based extraction (free, lower quality)
- `HybridPipeline` - Optimal balance (~$0.02/paper target)

Each pipeline implements:
```python
def extract(paper_text: str, paper_id: str) -> ExtractionResult
def get_metrics() -> PipelineMetrics
def get_description() -> str
def get_estimated_cost() -> float
```

**3. Type-Safe Data Models**
- `Entity` - Structured entity with `EntityType` enum
- `Relationship` - Typed relationship with `RelationshipType` enum
- `KnowledgeGraph` - Collection of entities and relationships
- `ExtractionResult` - Complete pipeline output with metrics

**4. Configuration Management**
- YAML-based configs: `src/config/llm_config.yaml`, `fetcher_config.yaml`
- Environment variables for API keys
- `Settings` class centralizes configuration access

**5. Modular Component Design**
```
Input ‚Üí Parser ‚Üí Pipeline ‚Üí Extractor ‚Üí Model ‚Üí Validator ‚Üí Output
```

---

## üì¶ Key Modules

### Core Data Models (`src/models/`)

**`entities.py`** - Core data structures:
- `EntityType` enum: FACT, HYPOTHESIS, EXPERIMENT, TECHNIQUE, RESULT, DATASET, ANALYSIS, CONCLUSION
- `RelationshipType` enum: HYPOTHESIS_TO_EXPERIMENT, METHOD_TO_RESULT, etc.
- `Entity` class: id, type, text, confidence, source_section, metadata
- `Relationship` class: source_id, target_id, relationship_type, confidence

**`graph.py`** - Knowledge graph structure:
- `KnowledgeGraph` class: paper_id, entities, relationships
- Conversion to NetworkX graphs for visualization

**`results.py`** - Pipeline outputs:
- `ExtractionResult`: paper_id, entities (grouped by type), relationships, metrics
- `PipelineMetrics`: processing_time, tokens_used, cost_usd, entities_extracted

### Extraction Pipelines (`src/pipelines/`)

**`base_pipeline.py`** - Abstract base class:
- Defines contract for all extraction pipelines
- Standardized interface for metrics collection

**`llm_pipeline.py`** - LLM-based extraction:
- Uses GPT-4o-mini (or configurable model) via OpenAI SDK
- Structured JSON output with few-shot prompting
- Cost: ~$0.03/paper (GPT-4o-mini) or ~$0.30/paper (GPT-4)
- Use case: Ground truth generation, high-quality baseline

### LLM Adapters (`src/llm_adapters/`)

**Factory-based LLM abstraction:**
- `base_adapter.py` - Abstract interface
- `openai_adapter.py` - OpenAI/ChatGPT implementation
- `ollama_adapter.py` - Local Ollama implementation
- `factory.py` - `get_llm_adapter(provider)` factory function

**Usage:**
```python
from src.llm_adapters import get_llm_adapter

llm = get_llm_adapter("openai")  # or "ollama"
result = llm.generate(prompt="...", system_prompt="...")
embeddings = llm.embed(["text1", "text2"])
```

### Paper Fetchers (`src/fetchers/`)

**Multi-source paper fetching:**
- `base_fetcher.py` - Abstract fetcher interface
- `pubmed_fetcher.py` - PubMed API implementation (E-utilities)
- `arxiv_fetcher.py` - arXiv API implementation (arxiv.py)
- `factory.py` - `get_fetcher(type)` factory

**Features:**
- Search by query: `fetcher.search("aging research", max_results=10)`
- Fetch metadata: `paper = fetcher.fetch_paper(pmid)` or `fetcher.fetch_paper(arxiv_id)`
- Download PDFs: Full-text PDF download (PubMed PMC, arXiv)
- Category search: `arxiv_fetcher.search_by_category(["cs.CL", "cs.AI"])`
- Article registry: Track downloaded papers in `articles/metadata.json`

**Supported Sources:**
- **PubMed/PMC:** Biomedical literature (NCBI E-utilities API)
- **arXiv:** Preprints in physics, CS, math, biology, etc.

### Document Parsers (`src/parsers/`)

**Multi-format document parsing:**
- `base_parser.py` - Abstract parser interface
- `pdf_parser.py` - PDF parsing using PyMuPDF (fitz)
  - Text extraction with layout preservation
  - Section detection (Abstract, Methods, Results, etc.)
  - Metadata extraction (title, authors, dates)
  - Optional table extraction via pdfplumber

**Section Detection:**
Automatically detects common paper sections using regex patterns:
- Abstract, Introduction, Methods, Results, Discussion, Conclusion, References

### Visualization (`src/visualization/`)

**`generate_svg.py`** - SVG knowledge graph generator:
- Hierarchical layout with entity types in columns
- Color-coded entities and relationships
- Bezier curve edges with arrow markers
- XML-safe text escaping
- Auto-sizing based on content

**Usage:**
```bash
python -m src.visualization.generate_svg results/output.json graph.svg
```

### Utilities (`src/utils/`)

**`article_registry.py`** - Article metadata tracking:
- SQLite-like JSON registry for downloaded papers
- Track PMID, arXiv ID, PMC ID, DOI, PDF path, download source
- Statistics: total articles, size, source breakdown
- Deduplication and lookup by any identifier (PMID, arXiv ID, PMC ID, DOI)

---

## üí° Cost Optimization Strategy

### Three-Pipeline Approach

**1. LLM Pipeline (Ground Truth)**
- Model: GPT-4 or GPT-4o-mini
- Cost: $0.03-$0.30 per paper
- Precision: ~95% (expected)
- Use: Create 10-15 annotated papers as ground truth

**2. Regex Pipeline (Baseline)**
- Cost: $0.00 (CPU only)
- Precision: ~60-70% (expected)
- Speed: 200-300 papers/hour
- Use: Fast processing, simple pattern matching

**3. Hybrid Pipeline (Production Target)**
- Cost: ~$0.02 per paper
- Precision: ‚â•85% (target)
- Strategy:
  1. Regex for simple patterns (Methods, Results)
  2. NLP (spaCy) for entity recognition (Facts)
  3. Selective LLM for complex reasoning (Hypotheses, Conclusions)

**Decision Algorithm:**
```
If regex confidence > 0.8:
    Use regex result (FREE)
Elif entity_type in [facts, techniques]:
    Use NLP extractor (~$0.001/paper)
Elif entity_type in [hypotheses, conclusions]:
    Use LLM selectively (~$0.01/paper)
```

### Optimization Techniques
1. **Batch processing** - Combine multiple sections into single API call
2. **Caching** - LRU cache for identical text segments
3. **Model selection** - GPT-4o-mini instead of GPT-4 (20x cheaper)
4. **Chunking** - Process only relevant sections, not full papers

---

## üõ†Ô∏è –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Å—Ç–µ–∫

### Core (Existing)
- **Python 3.10+**
- **OpenAI SDK** - GPT-4o-mini for LLM extraction
- **PyMuPDF (fitz)** - PDF text extraction
- **pdfplumber** - PDF table extraction
- **requests** - HTTP client for API calls
- **arxiv** - arXiv API wrapper for paper fetching
- **python-dotenv** - Environment variable management
- **pyyaml** - YAML configuration parsing

### Testing
- **pytest** - Test framework
- **pytest-cov** - Coverage reporting

### Future Dependencies (Planned)
```python
# Will be added as needed:
chromadb>=0.4.0          # Vector database
streamlit>=1.28.0        # Web UI
spacy>=3.7.0             # NLP for hybrid pipeline
scispacy>=0.5.0          # Scientific text processing
networkx>=3.2.0          # Graph processing
plotly>=5.17.0           # Interactive visualizations
```

---

## üéØ –ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏ –∂—é—Ä–∏

### 1. –ü–æ–ª–Ω–æ—Ç–∞ –∏ –¢–æ—á–Ω–æ—Å—Ç—å (25%)
- **Precision:** % –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
- **Recall:** % –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –æ—Ç –≤—Å–µ—Ö —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö
- **F1 Score:** –ì–∞—Ä–º–æ–Ω–∏—á–µ—Å–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ precision –∏ recall

### 2. –†–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å (25%)
- **–§–æ—Ä–º–∞—Ç—ã:** –û–±—Ä–∞–±–æ—Ç–∫–∞ PDF, HTML, XML
- **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å:** –†–∞–±–æ—Ç–∞ —Å —Ä–∞–∑–Ω—ã–º–∏ –∂—É—Ä–Ω–∞–ª–∞–º–∏ –∏ —Å—Ç–∏–ª—è–º–∏ –Ω–∞–ø–∏—Å–∞–Ω–∏—è
- **Error Handling:** –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ –æ—à–∏–±–æ–∫

### 3. –°—Ç–æ–∏–º–æ—Å—Ç–Ω—ã–π –ê–Ω–∞–ª–∏–∑ (25%)
- **CPU/GPU —á–∞—Å—ã:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã
- **$/—Å—Ç–∞—Ç—å—è:** –û–±—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏
- **Tokens used:** –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è API
- **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å:** –ü—Ä–æ–µ–∫—Ü–∏—è –Ω–∞ 50M —Å—Ç–∞—Ç–µ–π

### 4. –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (25%)
- **Throughput:** –°—Ç–∞—Ç–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤ —á–∞—Å
- **Latency:** –í—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞
- **Parallelization:** –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ

---

## ‚≠ê BONUS POINTS

1. **–ê–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–µ –∏–ª–∏ –≥–∏–±—Ä–∏–¥–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è** —Å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º —Å–Ω–∏–∂–µ–Ω–∏–µ–º —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞

2. **–£–ª—É—á—à–µ–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ–≥–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞:**
   - –ù–æ–≤—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
   - –£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–≤—è–∑–µ–π
   - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≥—Ä–∞—Ñ–∞
   - –ö—Ä–µ–∞—Ç–∏–≤–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏

---

## üìù –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø–æ–¥–∞—á–µ

‚úÖ **–û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ:**
- üé• **–í–∏–¥–µ–æ-–¥–µ–º–æ** (3-5 –º–∏–Ω—É—Ç)
- üíª **–û—Ç–∫—Ä—ã—Ç—ã–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π** —Å README
- üåê **–†–∞–∑–≤–µ—Ä–Ω—É—Ç–æ–µ —Ä–µ—à–µ–Ω–∏–µ** (–ø—É–±–ª–∏—á–Ω—ã–π URL)
- üìÑ **–û–ø–∏—Å–∞–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–∞** - –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è
- üìä **Performance metrics** - Precision, Recall, F1, Throughput, Cost
- üí∞ **Cost analysis** - –î–µ—Ç–∞–ª—å–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞

‚ö†Ô∏è **–í–∞–∂–Ω–æ:** –ñ—é—Ä–∏ –ù–ï –±—É–¥–µ—Ç –∑–∞–ø—É—Å–∫–∞—Ç—å –∫–æ–¥ –ª–æ–∫–∞–ª—å–Ω–æ!

---

## üîó –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

- **–ü–æ–ª–Ω–∞—è —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è:** `docs/singularis_project_doc.md`
- **Pipeline –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** `docs/pipeline_architecture_plan.md`
- **PubMed API reference:** `docs/pubmed_api_reference.md`

---

## üìû –ü–æ–¥–¥–µ—Ä–∂–∫–∞

- **Discord:** #singularis-challenge
- **–ú–µ–Ω—Ç–æ—Ä—ã:** –î–æ—Å—Ç—É–ø–Ω—ã —á–µ—Ä–µ–∑ Discord
- **–í–æ–ø—Ä–æ—Å—ã:** DM @HackAging.ai

---

**–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ:** 11 –æ–∫—Ç—è–±—Ä—è 2025
**–°—Ç–∞—Ç—É—Å:** Active Development - Week 1 (MVP)
